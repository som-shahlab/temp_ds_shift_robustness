{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import scipy\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display\n",
    "from prediction_utils.pytorch_utils.metrics import (\n",
    "    StandardEvaluator,\n",
    "    FairOVAEvaluator\n",
    ")\n",
    "from prediction_utils.pytorch_utils.datasets import ArrayLoaderGenerator\n",
    "from prediction_utils.pytorch_utils.models import TorchModel, FixedWidthModel\n",
    "from prediction_utils.util import patient_split\n",
    "from prediction_utils.pytorch_utils.group_fairness import group_regularized_model\n",
    "from prediction_utils.pytorch_utils.lagrangian import group_lagrangian_model\n",
    "from prediction_utils.pytorch_utils.robustness import group_robust_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_survival</th>\n",
       "      <th>prob_y</th>\n",
       "      <th>prob_censored</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.708497</td>\n",
       "      <td>0.815856</td>\n",
       "      <td>0.138643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.188879</td>\n",
       "      <td>0.609008</td>\n",
       "      <td>0.319714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_survival    prob_y  prob_censored\n",
       "group                                        \n",
       "0           5.708497  0.815856       0.138643\n",
       "1          12.188879  0.609008       0.319714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simulation(\n",
    "    n=10000, \n",
    "    num_features=50,\n",
    "    survival_group_means = [4, 15],\n",
    "    censoring_group_means = [5, 5],\n",
    "    binary_event_horizon=10,\n",
    "    max_weight=100\n",
    "):\n",
    "    eps = 1e-6\n",
    "    # Get randomness\n",
    "    generator = np.random.default_rng()\n",
    "    \n",
    "    # Convert inputs to numpy\n",
    "    survival_group_means = np.array(survival_group_means)\n",
    "    censoring_group_means = np.array(censoring_group_means)\n",
    "    num_groups = len(survival_group_means)\n",
    "    num_groups = len(survival_group_means)\n",
    "    assert num_groups == len(censoring_group_means)\n",
    "\n",
    "    group = generator.integers(num_groups, size=n)\n",
    "    \n",
    "    features = generator.standard_normal(size=(n, num_features))\n",
    "    weights = generator.standard_normal(size=(num_features, num_groups))\n",
    "    \n",
    "    survival_mean_shifts = survival_group_means[group]\n",
    "    survival_means = np.maximum(\n",
    "        np.dot(features, weights)[np.arange(n), group].reshape(-1) + survival_mean_shifts, eps\n",
    "    )\n",
    "    \n",
    "    survival_times = generator.exponential(survival_means)\n",
    "    \n",
    "    censoring_mean_shifts = censoring_group_means[group]\n",
    "    censoring_means = np.maximum(\n",
    "        np.dot(features, weights)[np.arange(n), group].reshape(-1) + censoring_mean_shifts, eps\n",
    "    )\n",
    "    \n",
    "    censoring_times = generator.exponential(censoring_means)\n",
    "    \n",
    "    # The observed times\n",
    "    observed_times = np.minimum(survival_times, censoring_times)\n",
    "    \n",
    "    # The observed times when also censored by the binary event horizon\n",
    "    binary_observed_times = np.minimum(observed_times, binary_event_horizon)\n",
    "    \n",
    "    # Indicators for whether the observed time is an event or censoring\n",
    "    event_indicator = survival_times <= censoring_times\n",
    "    censored_indicator = ~event_indicator\n",
    "    \n",
    "    survival_function = np.exp(-observed_times / survival_means)\n",
    "    \n",
    "    # The survival function for the censoring process\n",
    "    censoring_survival_function = np.exp(-observed_times / (censoring_means))\n",
    "    \n",
    "    # The survival function for the censoring process when also censored by the binary event horizon\n",
    "    binary_censoring_survival_function = np.exp(-binary_observed_times / (censoring_means))\n",
    "    \n",
    "    # The true value of the binary outcome Y\n",
    "    true_labels = (survival_times < binary_event_horizon)\n",
    "    \n",
    "    labels = true_labels\n",
    "    \n",
    "    # An indicator for whether the binary outcome is observed\n",
    "    binary_event_indicator = ((observed_times < binary_event_horizon) & (event_indicator)) | (binary_event_horizon <= observed_times)\n",
    "    \n",
    "    observed_labels = ((observed_times < binary_event_horizon) & (event_indicator))\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'row_id': np.arange(n),\n",
    "            'survival_times': survival_times,\n",
    "            'censoring_times': censoring_times,\n",
    "            'observed_times': observed_times,\n",
    "            'binary_observed_times': binary_observed_times,\n",
    "            'group': group,\n",
    "            'event_indicator': event_indicator,\n",
    "            'censored_indicator': censored_indicator,\n",
    "            'binary_event_indicator': binary_event_indicator,\n",
    "            'true_labels': true_labels,\n",
    "            'labels': labels,\n",
    "            'observed_labels': observed_labels,\n",
    "            'survival_function': survival_function,\n",
    "            'censoring_survival_function': censoring_survival_function,\n",
    "            'censoring_weight': np.minimum(1 / censoring_survival_function, max_weight),\n",
    "            'binary_censoring_weight': np.minimum(1 / binary_censoring_survival_function, max_weight)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return df, features\n",
    "    \n",
    "df, features = simulation(\n",
    "    100000, \n",
    "    max_weight=1e6,\n",
    "    survival_group_means = [5, 12],\n",
    "    censoring_group_means = [20, 15]\n",
    ")\n",
    "\n",
    "fold_id = '1'\n",
    "df = patient_split(df, patient_col='row_id'\n",
    "                  )\n",
    "\n",
    "display(df.groupby('group').agg(\n",
    "    mean_survival = ('survival_times', 'mean'), \n",
    "    prob_y = ('true_labels', 'mean'),\n",
    "    prob_censored = ('binary_event_indicator', lambda x: 1-x.mean())\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_merge(x):\n",
    "    for i, (key, value) in enumerate(x.items()):\n",
    "        if i == 0:\n",
    "            result = value\n",
    "        else:\n",
    "            result = result.merge(value)\n",
    "    return result\n",
    "\n",
    "def temp_evaluate(df, strata_vars=None, pred_prob_var='pred_probs'):\n",
    "    evaluator = StandardEvaluator(thresholds=[0.1, 0.5, 0.9])\n",
    "    result_dict = {\n",
    "        'unbiased': evaluator.evaluate(\n",
    "            df, \n",
    "            strata_vars=strata_vars, \n",
    "            label_var = 'labels', \n",
    "            pred_prob_var=pred_prob_var\n",
    "        ),\n",
    "        'adjusted': evaluator.evaluate(\n",
    "            df.query('binary_event_indicator == 1'), \n",
    "            strata_vars=strata_vars, \n",
    "            result_name='performance_ipcw', \n",
    "            weight_var='binary_censoring_weight', \n",
    "            label_var = 'observed_labels', \n",
    "            pred_prob_var=pred_prob_var\n",
    "        ),\n",
    "        'biased_obs': evaluator.evaluate(\n",
    "            df.query('binary_event_indicator == 1'), \n",
    "            strata_vars=strata_vars, \n",
    "            result_name='performance_biased_obs',\n",
    "            label_var = 'observed_labels', \n",
    "            pred_prob_var=pred_prob_var\n",
    "        ),\n",
    "        'biased_neg': evaluator.evaluate(\n",
    "            df, \n",
    "            label_var = 'observed_labels', \n",
    "            strata_vars=strata_vars, \n",
    "            result_name='performance_biased_neg', \n",
    "            pred_prob_var=pred_prob_var\n",
    "        )\n",
    "    }\n",
    "    return concat_merge(result_dict)\n",
    "\n",
    "\n",
    "def temp_evaluate_fair(df, group_var_name='group', pred_prob_var='pred_probs'):\n",
    "    evaluator = FairOVAEvaluator(thresholds=[0.1, 0.5, 0.9])\n",
    "    \n",
    "    result_dict = {\n",
    "        'unbiased': evaluator.evaluate(\n",
    "            df, \n",
    "            group_var_name=group_var_name, \n",
    "            label_var = 'labels', \n",
    "            pred_prob_var=pred_prob_var\n",
    "        ),\n",
    "        'adjusted': evaluator.evaluate(\n",
    "            df.query('binary_event_indicator == 1'), \n",
    "            group_var_name=group_var_name, \n",
    "            result_name='performance_ipcw', \n",
    "            weight_var='binary_censoring_weight', \n",
    "            label_var = 'observed_labels', \n",
    "            pred_prob_var=pred_prob_var\n",
    "        ),\n",
    "        'biased_obs': evaluator.evaluate(\n",
    "            df.query('binary_event_indicator == 1'), \n",
    "            group_var_name=group_var_name, \n",
    "            result_name='performance_biased_obs', \n",
    "            label_var = 'observed_labels', \n",
    "            pred_prob_var=pred_prob_var\n",
    "        ),\n",
    "        'biased_neg': evaluator.evaluate(\n",
    "            df, \n",
    "            label_var = 'observed_labels', \n",
    "            group_var_name=group_var_name, \n",
    "            result_name='performance_biased_neg', \n",
    "            pred_prob_var=pred_prob_var\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return concat_merge(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df_dict = {\n",
    "    'train': df.query('(fold_id != \"test\") & (fold_id != @fold_id)'),\n",
    "    'val': df.query('fold_id == @fold_id'),\n",
    "    'test': df.query('fold_id == \"test\"')\n",
    "}\n",
    "features_dict = {\n",
    "    key: np.concatenate(\n",
    "        (\n",
    "            features[value.row_id, ], \n",
    "            value.group.values.reshape(-1, 1)\n",
    "        ), \n",
    "        axis=1\n",
    "    ) \n",
    "    for key, value in labels_df_dict.items()\n",
    "}\n",
    "\n",
    "config_dict = {\n",
    "    'lr': 1e-5, \n",
    "    'batch_size': 256, \n",
    "    'num_epochs': 20, \n",
    "    'print_every': 10,\n",
    "    'verbose': False,\n",
    "    'sparse': False,\n",
    "    'input_dim': features.shape[1] + 1,\n",
    "    'include_group_in_dataset': True,\n",
    "    'logging_evaluate_by_group': True,\n",
    "    'group_var_name': 'group',\n",
    "    'compute_group_min_max':True,\n",
    "    'selection_metric': 'auc_min',\n",
    "    'early_stopping': True,\n",
    "    'early_stopping_patience': 10,\n",
    "    'num_groups': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "cuda\n",
      "Epoch 0/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.480775\n",
      "1                   auc    1.0     0.511709\n",
      "2                 auprc    0.0     0.848429\n",
      "3                 auprc    1.0     0.693107\n",
      "4                 brier    0.0     0.146412\n",
      "5                 brier    1.0     0.235342\n",
      "6              loss_bce    0.0     0.474940\n",
      "7              loss_bce    1.0     0.695305\n",
      "8       specificity_0.5    0.0     0.019627\n",
      "9       specificity_0.5    1.0     0.044608\n",
      "10        precision_0.5    0.0     0.852800\n",
      "11        precision_0.5    1.0     0.688599\n",
      "12           recall_0.5    0.0     0.973419\n",
      "13           recall_0.5    1.0     0.961825\n",
      "0                  loss    NaN     0.572400\n",
      "0               auc_min    NaN     0.480775\n",
      "1             auprc_min    NaN     0.693107\n",
      "2             brier_min    NaN     0.146412\n",
      "3          loss_bce_min    NaN     0.474940\n",
      "4     precision_0.5_min    NaN     0.688599\n",
      "5        recall_0.5_min    NaN     0.961825\n",
      "6   specificity_0.5_min    NaN     0.019627\n",
      "7               auc_max    NaN     0.511709\n",
      "8             auprc_max    NaN     0.848429\n",
      "9             brier_max    NaN     0.235342\n",
      "10         loss_bce_max    NaN     0.695305\n",
      "11    precision_0.5_max    NaN     0.852800\n",
      "12       recall_0.5_max    NaN     0.973419\n",
      "13  specificity_0.5_max    NaN     0.044608\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.514298\n",
      "1                   auc    1.0     0.508012\n",
      "2                 auprc    0.0     0.872225\n",
      "3                 auprc    1.0     0.685125\n",
      "4                 brier    0.0     0.133915\n",
      "5                 brier    1.0     0.244707\n",
      "6              loss_bce    0.0     0.441654\n",
      "7              loss_bce    1.0     0.714906\n",
      "8       specificity_0.5    0.0     0.036893\n",
      "9       specificity_0.5    1.0     0.034659\n",
      "10        precision_0.5    0.0     0.869782\n",
      "11        precision_0.5    1.0     0.671066\n",
      "12           recall_0.5    0.0     0.974985\n",
      "13           recall_0.5    1.0     0.957384\n",
      "0                  loss    NaN     0.560213\n",
      "0               auc_min    NaN     0.508012\n",
      "1             auprc_min    NaN     0.685125\n",
      "2             brier_min    NaN     0.133915\n",
      "3          loss_bce_min    NaN     0.441654\n",
      "4     precision_0.5_min    NaN     0.671066\n",
      "5        recall_0.5_min    NaN     0.957384\n",
      "6   specificity_0.5_min    NaN     0.034659\n",
      "7               auc_max    NaN     0.514298\n",
      "8             auprc_max    NaN     0.872225\n",
      "9             brier_max    NaN     0.244707\n",
      "10         loss_bce_max    NaN     0.714906\n",
      "11    precision_0.5_max    NaN     0.869782\n",
      "12       recall_0.5_max    NaN     0.974985\n",
      "13  specificity_0.5_max    NaN     0.036893\n",
      "Epoch 10/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.567922\n",
      "1                   auc    1.0     0.576273\n",
      "2                 auprc    0.0     0.874466\n",
      "3                 auprc    1.0     0.734091\n",
      "4                 brier    0.0     0.138670\n",
      "5                 brier    1.0     0.220217\n",
      "6              loss_bce    0.0     0.448942\n",
      "7              loss_bce    1.0     0.644078\n",
      "8       specificity_0.5    0.0     0.018059\n",
      "9       specificity_0.5    1.0     0.043345\n",
      "10        precision_0.5    0.0     0.845427\n",
      "11        precision_0.5    1.0     0.689564\n",
      "12           recall_0.5    0.0     0.987958\n",
      "13           recall_0.5    1.0     0.978244\n",
      "0                  loss    NaN     0.535411\n",
      "0               auc_min    NaN     0.567922\n",
      "1             auprc_min    NaN     0.734091\n",
      "2             brier_min    NaN     0.138670\n",
      "3          loss_bce_min    NaN     0.448942\n",
      "4     precision_0.5_min    NaN     0.689564\n",
      "5        recall_0.5_min    NaN     0.978244\n",
      "6   specificity_0.5_min    NaN     0.018059\n",
      "7               auc_max    NaN     0.576273\n",
      "8             auprc_max    NaN     0.874466\n",
      "9             brier_max    NaN     0.220217\n",
      "10         loss_bce_max    NaN     0.644078\n",
      "11    precision_0.5_max    NaN     0.845427\n",
      "12       recall_0.5_max    NaN     0.987958\n",
      "13  specificity_0.5_max    NaN     0.043345\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.605586\n",
      "1                   auc    1.0     0.569286\n",
      "2                 auprc    0.0     0.900737\n",
      "3                 auprc    1.0     0.730601\n",
      "4                 brier    0.0     0.121934\n",
      "5                 brier    1.0     0.228990\n",
      "6              loss_bce    0.0     0.407751\n",
      "7              loss_bce    1.0     0.662362\n",
      "8       specificity_0.5    0.0     0.031068\n",
      "9       specificity_0.5    1.0     0.042813\n",
      "10        precision_0.5    0.0     0.870993\n",
      "11        precision_0.5    1.0     0.676764\n",
      "12           recall_0.5    0.0     0.991466\n",
      "13           recall_0.5    1.0     0.974232\n",
      "0                  loss    NaN     0.518223\n",
      "0               auc_min    NaN     0.569286\n",
      "1             auprc_min    NaN     0.730601\n",
      "2             brier_min    NaN     0.121934\n",
      "3          loss_bce_min    NaN     0.407751\n",
      "4     precision_0.5_min    NaN     0.676764\n",
      "5        recall_0.5_min    NaN     0.974232\n",
      "6   specificity_0.5_min    NaN     0.031068\n",
      "7               auc_max    NaN     0.605586\n",
      "8             auprc_max    NaN     0.900737\n",
      "9             brier_max    NaN     0.228990\n",
      "10         loss_bce_max    NaN     0.662362\n",
      "11    precision_0.5_max    NaN     0.870993\n",
      "12       recall_0.5_max    NaN     0.991466\n",
      "13  specificity_0.5_max    NaN     0.042813\n",
      "Epoch 20/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.648852\n",
      "1                   auc    1.0     0.608225\n",
      "2                 auprc    0.0     0.908503\n",
      "3                 auprc    1.0     0.757026\n",
      "4                 brier    0.0     0.126131\n",
      "5                 brier    1.0     0.216025\n",
      "6              loss_bce    0.0     0.413860\n",
      "7              loss_bce    1.0     0.629049\n",
      "8       specificity_0.5    0.0     0.027514\n",
      "9       specificity_0.5    1.0     0.062947\n",
      "10        precision_0.5    0.0     0.854403\n",
      "11        precision_0.5    1.0     0.687992\n",
      "12           recall_0.5    0.0     0.990776\n",
      "13           recall_0.5    1.0     0.974715\n",
      "0                  loss    NaN     0.509266\n",
      "0               auc_min    NaN     0.608225\n",
      "1             auprc_min    NaN     0.757026\n",
      "2             brier_min    NaN     0.126131\n",
      "3          loss_bce_min    NaN     0.413860\n",
      "4     precision_0.5_min    NaN     0.687992\n",
      "5        recall_0.5_min    NaN     0.974715\n",
      "6   specificity_0.5_min    NaN     0.027514\n",
      "7               auc_max    NaN     0.648852\n",
      "8             auprc_max    NaN     0.908503\n",
      "9             brier_max    NaN     0.216025\n",
      "10         loss_bce_max    NaN     0.629049\n",
      "11    precision_0.5_max    NaN     0.854403\n",
      "12       recall_0.5_max    NaN     0.990776\n",
      "13  specificity_0.5_max    NaN     0.062947\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.669079\n",
      "1                   auc    1.0     0.613774\n",
      "2                 auprc    0.0     0.920521\n",
      "3                 auprc    1.0     0.763865\n",
      "4                 brier    0.0     0.115358\n",
      "5                 brier    1.0     0.219590\n",
      "6              loss_bce    0.0     0.388178\n",
      "7              loss_bce    1.0     0.634767\n",
      "8       specificity_0.5    0.0     0.044660\n",
      "9       specificity_0.5    1.0     0.054027\n",
      "10        precision_0.5    0.0     0.872835\n",
      "11        precision_0.5    1.0     0.678559\n",
      "12           recall_0.5    0.0     0.993820\n",
      "13           recall_0.5    1.0     0.970763\n",
      "0                  loss    NaN     0.495169\n",
      "0               auc_min    NaN     0.613774\n",
      "1             auprc_min    NaN     0.763865\n",
      "2             brier_min    NaN     0.115358\n",
      "3          loss_bce_min    NaN     0.388178\n",
      "4     precision_0.5_min    NaN     0.678559\n",
      "5        recall_0.5_min    NaN     0.970763\n",
      "6   specificity_0.5_min    NaN     0.044660\n",
      "7               auc_max    NaN     0.669079\n",
      "8             auprc_max    NaN     0.920521\n",
      "9             brier_max    NaN     0.219590\n",
      "10         loss_bce_max    NaN     0.634767\n",
      "11    precision_0.5_max    NaN     0.872835\n",
      "12       recall_0.5_max    NaN     0.993820\n",
      "13  specificity_0.5_max    NaN     0.054027\n",
      "Epoch 30/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.677052\n",
      "1                   auc    1.0     0.646792\n",
      "2                 auprc    0.0     0.922987\n",
      "3                 auprc    1.0     0.785473\n",
      "4                 brier    0.0     0.119922\n",
      "5                 brier    1.0     0.209210\n",
      "6              loss_bce    0.0     0.396009\n",
      "7              loss_bce    1.0     0.609622\n",
      "8       specificity_0.5    0.0     0.028051\n",
      "9       specificity_0.5    1.0     0.098533\n",
      "10        precision_0.5    0.0     0.861082\n",
      "11        precision_0.5    1.0     0.693517\n",
      "12           recall_0.5    0.0     0.989812\n",
      "13           recall_0.5    1.0     0.971398\n",
      "0                  loss    NaN     0.489465\n",
      "0               auc_min    NaN     0.646792\n",
      "1             auprc_min    NaN     0.785473\n",
      "2             brier_min    NaN     0.119922\n",
      "3          loss_bce_min    NaN     0.396009\n",
      "4     precision_0.5_min    NaN     0.693517\n",
      "5        recall_0.5_min    NaN     0.971398\n",
      "6   specificity_0.5_min    NaN     0.028051\n",
      "7               auc_max    NaN     0.677052\n",
      "8             auprc_max    NaN     0.922987\n",
      "9             brier_max    NaN     0.209210\n",
      "10         loss_bce_max    NaN     0.609622\n",
      "11    precision_0.5_max    NaN     0.861082\n",
      "12       recall_0.5_max    NaN     0.989812\n",
      "13  specificity_0.5_max    NaN     0.098533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.702152\n",
      "1                   auc    1.0     0.638998\n",
      "2                 auprc    0.0     0.931025\n",
      "3                 auprc    1.0     0.781701\n",
      "4                 brier    0.0     0.111518\n",
      "5                 brier    1.0     0.214469\n",
      "6              loss_bce    0.0     0.375776\n",
      "7              loss_bce    1.0     0.621137\n",
      "8       specificity_0.5    0.0     0.056311\n",
      "9       specificity_0.5    1.0     0.080530\n",
      "10        precision_0.5    0.0     0.874028\n",
      "11        precision_0.5    1.0     0.683287\n",
      "12           recall_0.5    0.0     0.992348\n",
      "13           recall_0.5    1.0     0.964321\n",
      "0                  loss    NaN     0.482234\n",
      "0               auc_min    NaN     0.638998\n",
      "1             auprc_min    NaN     0.781701\n",
      "2             brier_min    NaN     0.111518\n",
      "3          loss_bce_min    NaN     0.375776\n",
      "4     precision_0.5_min    NaN     0.683287\n",
      "5        recall_0.5_min    NaN     0.964321\n",
      "6   specificity_0.5_min    NaN     0.056311\n",
      "7               auc_max    NaN     0.702152\n",
      "8             auprc_max    NaN     0.931025\n",
      "9             brier_max    NaN     0.214469\n",
      "10         loss_bce_max    NaN     0.621137\n",
      "11    precision_0.5_max    NaN     0.874028\n",
      "12       recall_0.5_max    NaN     0.992348\n",
      "13  specificity_0.5_max    NaN     0.080530\n",
      "Epoch 40/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.694735\n",
      "1                   auc    1.0     0.655793\n",
      "2                 auprc    0.0     0.929561\n",
      "3                 auprc    1.0     0.796477\n",
      "4                 brier    0.0     0.118438\n",
      "5                 brier    1.0     0.207114\n",
      "6              loss_bce    0.0     0.389830\n",
      "7              loss_bce    1.0     0.603534\n",
      "8       specificity_0.5    0.0     0.043778\n",
      "9       specificity_0.5    1.0     0.113343\n",
      "10        precision_0.5    0.0     0.861420\n",
      "11        precision_0.5    1.0     0.695886\n",
      "12           recall_0.5    0.0     0.988224\n",
      "13           recall_0.5    1.0     0.957295\n",
      "0                  loss    NaN     0.484486\n",
      "0               auc_min    NaN     0.655793\n",
      "1             auprc_min    NaN     0.796477\n",
      "2             brier_min    NaN     0.118438\n",
      "3          loss_bce_min    NaN     0.389830\n",
      "4     precision_0.5_min    NaN     0.695886\n",
      "5        recall_0.5_min    NaN     0.957295\n",
      "6   specificity_0.5_min    NaN     0.043778\n",
      "7               auc_max    NaN     0.694735\n",
      "8             auprc_max    NaN     0.929561\n",
      "9             brier_max    NaN     0.207114\n",
      "10         loss_bce_max    NaN     0.603534\n",
      "11    precision_0.5_max    NaN     0.861420\n",
      "12       recall_0.5_max    NaN     0.988224\n",
      "13  specificity_0.5_max    NaN     0.113343\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.718288\n",
      "1                   auc    1.0     0.651169\n",
      "2                 auprc    0.0     0.936523\n",
      "3                 auprc    1.0     0.790065\n",
      "4                 brier    0.0     0.109455\n",
      "5                 brier    1.0     0.211558\n",
      "6              loss_bce    0.0     0.368426\n",
      "7              loss_bce    1.0     0.613743\n",
      "8       specificity_0.5    0.0     0.066019\n",
      "9       specificity_0.5    1.0     0.113150\n",
      "10        precision_0.5    0.0     0.875032\n",
      "11        precision_0.5    1.0     0.688172\n",
      "12           recall_0.5    0.0     0.991171\n",
      "13           recall_0.5    1.0     0.951437\n",
      "0                  loss    NaN     0.474865\n",
      "0               auc_min    NaN     0.651169\n",
      "1             auprc_min    NaN     0.790065\n",
      "2             brier_min    NaN     0.109455\n",
      "3          loss_bce_min    NaN     0.368426\n",
      "4     precision_0.5_min    NaN     0.688172\n",
      "5        recall_0.5_min    NaN     0.951437\n",
      "6   specificity_0.5_min    NaN     0.066019\n",
      "7               auc_max    NaN     0.718288\n",
      "8             auprc_max    NaN     0.936523\n",
      "9             brier_max    NaN     0.211558\n",
      "10         loss_bce_max    NaN     0.613743\n",
      "11    precision_0.5_max    NaN     0.875032\n",
      "12       recall_0.5_max    NaN     0.991171\n",
      "13  specificity_0.5_max    NaN     0.113150\n",
      "Epoch 50/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.716130\n",
      "1                   auc    1.0     0.652385\n",
      "2                 auprc    0.0     0.933977\n",
      "3                 auprc    1.0     0.792341\n",
      "4                 brier    0.0     0.117104\n",
      "5                 brier    1.0     0.208234\n",
      "6              loss_bce    0.0     0.384456\n",
      "7              loss_bce    1.0     0.607045\n",
      "8       specificity_0.5    0.0     0.048581\n",
      "9       specificity_0.5    1.0     0.126901\n",
      "10        precision_0.5    0.0     0.859766\n",
      "11        precision_0.5    1.0     0.696346\n",
      "12           recall_0.5    0.0     0.987460\n",
      "13           recall_0.5    1.0     0.950020\n",
      "0                  loss    NaN     0.482186\n",
      "0               auc_min    NaN     0.652385\n",
      "1             auprc_min    NaN     0.792341\n",
      "2             brier_min    NaN     0.117104\n",
      "3          loss_bce_min    NaN     0.384456\n",
      "4     precision_0.5_min    NaN     0.696346\n",
      "5        recall_0.5_min    NaN     0.950020\n",
      "6   specificity_0.5_min    NaN     0.048581\n",
      "7               auc_max    NaN     0.716130\n",
      "8             auprc_max    NaN     0.933977\n",
      "9             brier_max    NaN     0.208234\n",
      "10         loss_bce_max    NaN     0.607045\n",
      "11    precision_0.5_max    NaN     0.859766\n",
      "12       recall_0.5_max    NaN     0.987460\n",
      "13  specificity_0.5_max    NaN     0.126901\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.726132\n",
      "1                   auc    1.0     0.656964\n",
      "2                 auprc    0.0     0.939340\n",
      "3                 auprc    1.0     0.794322\n",
      "4                 brier    0.0     0.108174\n",
      "5                 brier    1.0     0.210018\n",
      "6              loss_bce    0.0     0.363584\n",
      "7              loss_bce    1.0     0.609920\n",
      "8       specificity_0.5    0.0     0.077670\n",
      "9       specificity_0.5    1.0     0.136595\n",
      "10        precision_0.5    0.0     0.876238\n",
      "11        precision_0.5    1.0     0.691776\n",
      "12           recall_0.5    0.0     0.989700\n",
      "13           recall_0.5    1.0     0.942022\n",
      "0                  loss    NaN     0.470465\n",
      "0               auc_min    NaN     0.656964\n",
      "1             auprc_min    NaN     0.794322\n",
      "2             brier_min    NaN     0.108174\n",
      "3          loss_bce_min    NaN     0.363584\n",
      "4     precision_0.5_min    NaN     0.691776\n",
      "5        recall_0.5_min    NaN     0.942022\n",
      "6   specificity_0.5_min    NaN     0.077670\n",
      "7               auc_max    NaN     0.726132\n",
      "8             auprc_max    NaN     0.939340\n",
      "9             brier_max    NaN     0.210018\n",
      "10         loss_bce_max    NaN     0.609920\n",
      "11    precision_0.5_max    NaN     0.876238\n",
      "12       recall_0.5_max    NaN     0.989700\n",
      "13  specificity_0.5_max    NaN     0.136595\n",
      "Epoch 60/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.720313\n",
      "1                   auc    1.0     0.661844\n",
      "2                 auprc    0.0     0.936231\n",
      "3                 auprc    1.0     0.800642\n",
      "4                 brier    0.0     0.114880\n",
      "5                 brier    1.0     0.206523\n",
      "6              loss_bce    0.0     0.378213\n",
      "7              loss_bce    1.0     0.601396\n",
      "8       specificity_0.5    0.0     0.051106\n",
      "9       specificity_0.5    1.0     0.152672\n",
      "10        precision_0.5    0.0     0.862298\n",
      "11        precision_0.5    1.0     0.697283\n",
      "12           recall_0.5    0.0     0.987666\n",
      "13           recall_0.5    1.0     0.935328\n",
      "0                  loss    NaN     0.476919\n",
      "0               auc_min    NaN     0.661844\n",
      "1             auprc_min    NaN     0.800642\n",
      "2             brier_min    NaN     0.114880\n",
      "3          loss_bce_min    NaN     0.378213\n",
      "4     precision_0.5_min    NaN     0.697283\n",
      "5        recall_0.5_min    NaN     0.935328\n",
      "6   specificity_0.5_min    NaN     0.051106\n",
      "7               auc_max    NaN     0.720313\n",
      "8             auprc_max    NaN     0.936231\n",
      "9             brier_max    NaN     0.206523\n",
      "10         loss_bce_max    NaN     0.601396\n",
      "11    precision_0.5_max    NaN     0.862298\n",
      "12       recall_0.5_max    NaN     0.987666\n",
      "13  specificity_0.5_max    NaN     0.152672\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.731175\n",
      "1                   auc    1.0     0.659734\n",
      "2                 auprc    0.0     0.941102\n",
      "3                 auprc    1.0     0.796161\n",
      "4                 brier    0.0     0.107307\n",
      "5                 brier    1.0     0.209041\n",
      "6              loss_bce    0.0     0.360233\n",
      "7              loss_bce    1.0     0.607523\n",
      "8       specificity_0.5    0.0     0.083495\n",
      "9       specificity_0.5    1.0     0.166157\n",
      "10        precision_0.5    0.0     0.876795\n",
      "11        precision_0.5    1.0     0.696812\n",
      "12           recall_0.5    0.0     0.988523\n",
      "13           recall_0.5    1.0     0.931615\n",
      "0                  loss    NaN     0.467528\n",
      "0               auc_min    NaN     0.659734\n",
      "1             auprc_min    NaN     0.796161\n",
      "2             brier_min    NaN     0.107307\n",
      "3          loss_bce_min    NaN     0.360233\n",
      "4     precision_0.5_min    NaN     0.696812\n",
      "5        recall_0.5_min    NaN     0.931615\n",
      "6   specificity_0.5_min    NaN     0.083495\n",
      "7               auc_max    NaN     0.731175\n",
      "8             auprc_max    NaN     0.941102\n",
      "9             brier_max    NaN     0.209041\n",
      "10         loss_bce_max    NaN     0.607523\n",
      "11    precision_0.5_max    NaN     0.876795\n",
      "12       recall_0.5_max    NaN     0.988523\n",
      "13  specificity_0.5_max    NaN     0.166157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.715886\n",
      "1                   auc    1.0     0.667754\n",
      "2                 auprc    0.0     0.933804\n",
      "3                 auprc    1.0     0.806339\n",
      "4                 brier    0.0     0.117482\n",
      "5                 brier    1.0     0.203835\n",
      "6              loss_bce    0.0     0.384087\n",
      "7              loss_bce    1.0     0.594834\n",
      "8       specificity_0.5    0.0     0.056992\n",
      "9       specificity_0.5    1.0     0.170504\n",
      "10        precision_0.5    0.0     0.858833\n",
      "11        precision_0.5    1.0     0.703827\n",
      "12           recall_0.5    0.0     0.987145\n",
      "13           recall_0.5    1.0     0.930392\n",
      "0                  loss    NaN     0.477746\n",
      "0               auc_min    NaN     0.667754\n",
      "1             auprc_min    NaN     0.806339\n",
      "2             brier_min    NaN     0.117482\n",
      "3          loss_bce_min    NaN     0.384087\n",
      "4     precision_0.5_min    NaN     0.703827\n",
      "5        recall_0.5_min    NaN     0.930392\n",
      "6   specificity_0.5_min    NaN     0.056992\n",
      "7               auc_max    NaN     0.715886\n",
      "8             auprc_max    NaN     0.933804\n",
      "9             brier_max    NaN     0.203835\n",
      "10         loss_bce_max    NaN     0.594834\n",
      "11    precision_0.5_max    NaN     0.858833\n",
      "12       recall_0.5_max    NaN     0.987145\n",
      "13  specificity_0.5_max    NaN     0.170504\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.735075\n",
      "1                   auc    1.0     0.661584\n",
      "2                 auprc    0.0     0.942534\n",
      "3                 auprc    1.0     0.797367\n",
      "4                 brier    0.0     0.106640\n",
      "5                 brier    1.0     0.208346\n",
      "6              loss_bce    0.0     0.357595\n",
      "7              loss_bce    1.0     0.605729\n",
      "8       specificity_0.5    0.0     0.085437\n",
      "9       specificity_0.5    1.0     0.180428\n",
      "10        precision_0.5    0.0     0.877023\n",
      "11        precision_0.5    1.0     0.699439\n",
      "12           recall_0.5    0.0     0.988523\n",
      "13           recall_0.5    1.0     0.927156\n",
      "0                  loss    NaN     0.465256\n",
      "0               auc_min    NaN     0.661584\n",
      "1             auprc_min    NaN     0.797367\n",
      "2             brier_min    NaN     0.106640\n",
      "3          loss_bce_min    NaN     0.357595\n",
      "4     precision_0.5_min    NaN     0.699439\n",
      "5        recall_0.5_min    NaN     0.927156\n",
      "6   specificity_0.5_min    NaN     0.085437\n",
      "7               auc_max    NaN     0.735075\n",
      "8             auprc_max    NaN     0.942534\n",
      "9             brier_max    NaN     0.208346\n",
      "10         loss_bce_max    NaN     0.605729\n",
      "11    precision_0.5_max    NaN     0.877023\n",
      "12       recall_0.5_max    NaN     0.988523\n",
      "13  specificity_0.5_max    NaN     0.180428\n",
      "Epoch 80/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.720331\n",
      "1                   auc    1.0     0.665823\n",
      "2                 auprc    0.0     0.934394\n",
      "3                 auprc    1.0     0.804655\n",
      "4                 brier    0.0     0.118241\n",
      "5                 brier    1.0     0.205762\n",
      "6              loss_bce    0.0     0.384966\n",
      "7              loss_bce    1.0     0.598935\n",
      "8       specificity_0.5    0.0     0.047732\n",
      "9       specificity_0.5    1.0     0.176068\n",
      "10        precision_0.5    0.0     0.856543\n",
      "11        precision_0.5    1.0     0.701251\n",
      "12           recall_0.5    0.0     0.985582\n",
      "13           recall_0.5    1.0     0.926295\n",
      "0                  loss    NaN     0.479221\n",
      "0               auc_min    NaN     0.665823\n",
      "1             auprc_min    NaN     0.804655\n",
      "2             brier_min    NaN     0.118241\n",
      "3          loss_bce_min    NaN     0.384966\n",
      "4     precision_0.5_min    NaN     0.701251\n",
      "5        recall_0.5_min    NaN     0.926295\n",
      "6   specificity_0.5_min    NaN     0.047732\n",
      "7               auc_max    NaN     0.720331\n",
      "8             auprc_max    NaN     0.934394\n",
      "9             brier_max    NaN     0.205762\n",
      "10         loss_bce_max    NaN     0.598935\n",
      "11    precision_0.5_max    NaN     0.856543\n",
      "12       recall_0.5_max    NaN     0.985582\n",
      "13  specificity_0.5_max    NaN     0.176068\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.738391\n",
      "1                   auc    1.0     0.662689\n",
      "2                 auprc    0.0     0.943713\n",
      "3                 auprc    1.0     0.798290\n",
      "4                 brier    0.0     0.106073\n",
      "5                 brier    1.0     0.207846\n",
      "6              loss_bce    0.0     0.355431\n",
      "7              loss_bce    1.0     0.604288\n",
      "8       specificity_0.5    0.0     0.083495\n",
      "9       specificity_0.5    1.0     0.189602\n",
      "10        precision_0.5    0.0     0.876795\n",
      "11        precision_0.5    1.0     0.700339\n",
      "12           recall_0.5    0.0     0.988523\n",
      "13           recall_0.5    1.0     0.920714\n",
      "0                  loss    NaN     0.463406\n",
      "0               auc_min    NaN     0.662689\n",
      "1             auprc_min    NaN     0.798290\n",
      "2             brier_min    NaN     0.106073\n",
      "3          loss_bce_min    NaN     0.355431\n",
      "4     precision_0.5_min    NaN     0.700339\n",
      "5        recall_0.5_min    NaN     0.920714\n",
      "6   specificity_0.5_min    NaN     0.083495\n",
      "7               auc_max    NaN     0.738391\n",
      "8             auprc_max    NaN     0.943713\n",
      "9             brier_max    NaN     0.207846\n",
      "10         loss_bce_max    NaN     0.604288\n",
      "11    precision_0.5_max    NaN     0.876795\n",
      "12       recall_0.5_max    NaN     0.988523\n",
      "13  specificity_0.5_max    NaN     0.189602\n",
      "Epoch 90/99\n",
      "----------\n",
      "Phase: train:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.729185\n",
      "1                   auc    1.0     0.667428\n",
      "2                 auprc    0.0     0.939430\n",
      "3                 auprc    1.0     0.798828\n",
      "4                 brier    0.0     0.113436\n",
      "5                 brier    1.0     0.207483\n",
      "6              loss_bce    0.0     0.372409\n",
      "7              loss_bce    1.0     0.603757\n",
      "8       specificity_0.5    0.0     0.069100\n",
      "9       specificity_0.5    1.0     0.182482\n",
      "10        precision_0.5    0.0     0.864047\n",
      "11        precision_0.5    1.0     0.696781\n",
      "12           recall_0.5    0.0     0.987973\n",
      "13           recall_0.5    1.0     0.921618\n",
      "0                  loss    NaN     0.473976\n",
      "0               auc_min    NaN     0.667428\n",
      "1             auprc_min    NaN     0.798828\n",
      "2             brier_min    NaN     0.113436\n",
      "3          loss_bce_min    NaN     0.372409\n",
      "4     precision_0.5_min    NaN     0.696781\n",
      "5        recall_0.5_min    NaN     0.921618\n",
      "6   specificity_0.5_min    NaN     0.069100\n",
      "7               auc_max    NaN     0.729185\n",
      "8             auprc_max    NaN     0.939430\n",
      "9             brier_max    NaN     0.207483\n",
      "10         loss_bce_max    NaN     0.603757\n",
      "11    precision_0.5_max    NaN     0.864047\n",
      "12       recall_0.5_max    NaN     0.987973\n",
      "13  specificity_0.5_max    NaN     0.182482\n",
      "Phase: val:\n",
      "                 metric  group  performance\n",
      "0                   auc    0.0     0.739911\n",
      "1                   auc    1.0     0.664135\n",
      "2                 auprc    0.0     0.944273\n",
      "3                 auprc    1.0     0.799293\n",
      "4                 brier    0.0     0.105510\n",
      "5                 brier    1.0     0.207480\n",
      "6              loss_bce    0.0     0.353343\n",
      "7              loss_bce    1.0     0.603337\n",
      "8       specificity_0.5    0.0     0.077670\n",
      "9       specificity_0.5    1.0     0.194699\n",
      "10        precision_0.5    0.0     0.876270\n",
      "11        precision_0.5    1.0     0.700984\n",
      "12           recall_0.5    0.0     0.989994\n",
      "13           recall_0.5    1.0     0.917740\n",
      "0                  loss    NaN     0.461811\n",
      "0               auc_min    NaN     0.664135\n",
      "1             auprc_min    NaN     0.799293\n",
      "2             brier_min    NaN     0.105510\n",
      "3          loss_bce_min    NaN     0.353343\n",
      "4     precision_0.5_min    NaN     0.700984\n",
      "5        recall_0.5_min    NaN     0.917740\n",
      "6   specificity_0.5_min    NaN     0.077670\n",
      "7               auc_max    NaN     0.739911\n",
      "8             auprc_max    NaN     0.944273\n",
      "9             brier_max    NaN     0.207480\n",
      "10         loss_bce_max    NaN     0.603337\n",
      "11    precision_0.5_max    NaN     0.876270\n",
      "12       recall_0.5_max    NaN     0.989994\n",
      "13  specificity_0.5_max    NaN     0.194699\n",
      "Best performance: 0.665290\n",
      "CPU times: user 1min 13s, sys: 523 ms, total: 1min 13s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Unweighted Model\n",
    "loader_generator = ArrayLoaderGenerator(\n",
    "    features=np.concatenate((features.astype(np.float32), df.group.values.reshape(-1, 1).astype(np.float32)), axis=1), \n",
    "    cohort=df.query('binary_event_indicator == 1'), \n",
    "    label_col='observed_labels',\n",
    "    fold_id=fold_id,\n",
    "    eval_key='val', \n",
    "    row_id_col='row_id', \n",
    "    num_workers=0,\n",
    "    **config_dict\n",
    ")\n",
    "\n",
    "loaders = loader_generator.init_loaders()\n",
    "model = FixedWidthModel(**config_dict)\n",
    "result_dict = model.train(\n",
    "    loaders,\n",
    "#     logging_threshold_metrics=['specificity', 'recall'],\n",
    "#     logging_thresholds=[0.1, 0.5],\n",
    "    num_epochs=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Evaluating on phase: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-02fc7ce87d4b>:23: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ).assign(labels = lambda x: x.labels.astype(np.long))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>performance</th>\n",
       "      <th>performance_ipcw</th>\n",
       "      <th>performance_biased_obs</th>\n",
       "      <th>performance_biased_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.622285</td>\n",
       "      <td>0.621265</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>0.603093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auprc</td>\n",
       "      <td>0.798715</td>\n",
       "      <td>0.801957</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.683574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loss_bce</td>\n",
       "      <td>0.590732</td>\n",
       "      <td>0.587509</td>\n",
       "      <td>0.508092</td>\n",
       "      <td>0.741713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ace_rmse_logistic_log</td>\n",
       "      <td>0.061973</td>\n",
       "      <td>0.060066</td>\n",
       "      <td>0.041414</td>\n",
       "      <td>0.169586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ace_abs_logistic_log</td>\n",
       "      <td>0.051935</td>\n",
       "      <td>0.048313</td>\n",
       "      <td>0.035597</td>\n",
       "      <td>0.166235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>specificity_0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>specificity_0.5</td>\n",
       "      <td>0.033310</td>\n",
       "      <td>0.033888</td>\n",
       "      <td>0.036927</td>\n",
       "      <td>0.028035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>specificity_0.9</td>\n",
       "      <td>0.950210</td>\n",
       "      <td>0.952326</td>\n",
       "      <td>0.954735</td>\n",
       "      <td>0.936421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>precision_0.1</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>0.717697</td>\n",
       "      <td>0.781494</td>\n",
       "      <td>0.600500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>precision_0.5</td>\n",
       "      <td>0.718731</td>\n",
       "      <td>0.721580</td>\n",
       "      <td>0.785430</td>\n",
       "      <td>0.603856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>precision_0.9</td>\n",
       "      <td>0.859822</td>\n",
       "      <td>0.866017</td>\n",
       "      <td>0.908982</td>\n",
       "      <td>0.749260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>recall_0.1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>recall_0.5</td>\n",
       "      <td>0.985590</td>\n",
       "      <td>0.984883</td>\n",
       "      <td>0.985679</td>\n",
       "      <td>0.985679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>recall_0.9</td>\n",
       "      <td>0.121852</td>\n",
       "      <td>0.121208</td>\n",
       "      <td>0.126395</td>\n",
       "      <td>0.126395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   metric  performance  performance_ipcw  \\\n",
       "0                     auc     0.622285          0.621265   \n",
       "1                   auprc     0.798715          0.801957   \n",
       "2                loss_bce     0.590732          0.587509   \n",
       "3   ace_rmse_logistic_log     0.061973          0.060066   \n",
       "4    ace_abs_logistic_log     0.051935          0.048313   \n",
       "5         specificity_0.1     0.000000          0.000000   \n",
       "6         specificity_0.5     0.033310          0.033888   \n",
       "7         specificity_0.9     0.950210          0.952326   \n",
       "8           precision_0.1     0.714800          0.717697   \n",
       "9           precision_0.5     0.718731          0.721580   \n",
       "10          precision_0.9     0.859822          0.866017   \n",
       "11             recall_0.1     1.000000          1.000000   \n",
       "12             recall_0.5     0.985590          0.984883   \n",
       "13             recall_0.9     0.121852          0.121208   \n",
       "\n",
       "    performance_biased_obs  performance_biased_neg  \n",
       "0                 0.634921                0.603093  \n",
       "1                 0.856500                0.683574  \n",
       "2                 0.508092                0.741713  \n",
       "3                 0.041414                0.169586  \n",
       "4                 0.035597                0.166235  \n",
       "5                 0.000000                0.000000  \n",
       "6                 0.036927                0.028035  \n",
       "7                 0.954735                0.936421  \n",
       "8                 0.781494                0.600500  \n",
       "9                 0.785430                0.603856  \n",
       "10                0.908982                0.749260  \n",
       "11                1.000000                1.000000  \n",
       "12                0.985679                0.985679  \n",
       "13                0.126395                0.126395  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>group</th>\n",
       "      <th>performance</th>\n",
       "      <th>performance_ipcw</th>\n",
       "      <th>performance_biased_obs</th>\n",
       "      <th>performance_biased_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.645581</td>\n",
       "      <td>0.648762</td>\n",
       "      <td>0.659302</td>\n",
       "      <td>0.623265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc</td>\n",
       "      <td>1</td>\n",
       "      <td>0.574092</td>\n",
       "      <td>0.569253</td>\n",
       "      <td>0.578054</td>\n",
       "      <td>0.545134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auprc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.879789</td>\n",
       "      <td>0.883330</td>\n",
       "      <td>0.913797</td>\n",
       "      <td>0.809174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>auprc</td>\n",
       "      <td>1</td>\n",
       "      <td>0.686660</td>\n",
       "      <td>0.687934</td>\n",
       "      <td>0.753744</td>\n",
       "      <td>0.510867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loss_bce</td>\n",
       "      <td>0</td>\n",
       "      <td>0.465846</td>\n",
       "      <td>0.461858</td>\n",
       "      <td>0.408307</td>\n",
       "      <td>0.572183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loss_bce</td>\n",
       "      <td>1</td>\n",
       "      <td>0.713145</td>\n",
       "      <td>0.712078</td>\n",
       "      <td>0.632043</td>\n",
       "      <td>0.907886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ace_rmse_logistic_log</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.041428</td>\n",
       "      <td>0.074163</td>\n",
       "      <td>0.059499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ace_rmse_logistic_log</td>\n",
       "      <td>1</td>\n",
       "      <td>0.139105</td>\n",
       "      <td>0.138604</td>\n",
       "      <td>0.084505</td>\n",
       "      <td>0.288727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ace_abs_logistic_log</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035496</td>\n",
       "      <td>0.036080</td>\n",
       "      <td>0.065228</td>\n",
       "      <td>0.053629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ace_abs_logistic_log</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125650</td>\n",
       "      <td>0.122753</td>\n",
       "      <td>0.069958</td>\n",
       "      <td>0.276613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>specificity_0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>specificity_0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>specificity_0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>0.023608</td>\n",
       "      <td>0.024233</td>\n",
       "      <td>0.016768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>specificity_0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039583</td>\n",
       "      <td>0.038913</td>\n",
       "      <td>0.044340</td>\n",
       "      <td>0.033545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>specificity_0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.940987</td>\n",
       "      <td>0.950905</td>\n",
       "      <td>0.953150</td>\n",
       "      <td>0.926067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>specificity_0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954688</td>\n",
       "      <td>0.953021</td>\n",
       "      <td>0.955660</td>\n",
       "      <td>0.941483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>precision_0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811717</td>\n",
       "      <td>0.813805</td>\n",
       "      <td>0.854592</td>\n",
       "      <td>0.734949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>precision_0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619802</td>\n",
       "      <td>0.622417</td>\n",
       "      <td>0.690692</td>\n",
       "      <td>0.468713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>precision_0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.813787</td>\n",
       "      <td>0.816358</td>\n",
       "      <td>0.856770</td>\n",
       "      <td>0.736896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>precision_0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623597</td>\n",
       "      <td>0.625614</td>\n",
       "      <td>0.694788</td>\n",
       "      <td>0.470708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>precision_0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.914596</td>\n",
       "      <td>0.928283</td>\n",
       "      <td>0.949653</td>\n",
       "      <td>0.849379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>precision_0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.759214</td>\n",
       "      <td>0.818533</td>\n",
       "      <td>0.574526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>recall_0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>recall_0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>recall_0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>0.993071</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.993128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>recall_0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976038</td>\n",
       "      <td>0.974270</td>\n",
       "      <td>0.974229</td>\n",
       "      <td>0.974229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>recall_0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146590</td>\n",
       "      <td>0.145392</td>\n",
       "      <td>0.150357</td>\n",
       "      <td>0.150357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>recall_0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090096</td>\n",
       "      <td>0.089861</td>\n",
       "      <td>0.089565</td>\n",
       "      <td>0.089565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   metric  group  performance  performance_ipcw  \\\n",
       "0                     auc      0     0.645581          0.648762   \n",
       "1                     auc      1     0.574092          0.569253   \n",
       "2                   auprc      0     0.879789          0.883330   \n",
       "3                   auprc      1     0.686660          0.687934   \n",
       "4                loss_bce      0     0.465846          0.461858   \n",
       "5                loss_bce      1     0.713145          0.712078   \n",
       "6   ace_rmse_logistic_log      0     0.040859          0.041428   \n",
       "7   ace_rmse_logistic_log      1     0.139105          0.138604   \n",
       "8    ace_abs_logistic_log      0     0.035496          0.036080   \n",
       "9    ace_abs_logistic_log      1     0.125650          0.122753   \n",
       "10        specificity_0.1      0     0.000000          0.000000   \n",
       "11        specificity_0.1      1     0.000000          0.000000   \n",
       "12        specificity_0.5      0     0.020386          0.023608   \n",
       "13        specificity_0.5      1     0.039583          0.038913   \n",
       "14        specificity_0.9      0     0.940987          0.950905   \n",
       "15        specificity_0.9      1     0.954688          0.953021   \n",
       "16          precision_0.1      0     0.811717          0.813805   \n",
       "17          precision_0.1      1     0.619802          0.622417   \n",
       "18          precision_0.5      0     0.813787          0.816358   \n",
       "19          precision_0.5      1     0.623597          0.625614   \n",
       "20          precision_0.9      0     0.914596          0.928283   \n",
       "21          precision_0.9      1     0.764228          0.759214   \n",
       "22             recall_0.1      0     1.000000          1.000000   \n",
       "23             recall_0.1      1     1.000000          1.000000   \n",
       "24             recall_0.5      0     0.993031          0.993071   \n",
       "25             recall_0.5      1     0.976038          0.974270   \n",
       "26             recall_0.9      0     0.146590          0.145392   \n",
       "27             recall_0.9      1     0.090096          0.089861   \n",
       "\n",
       "    performance_biased_obs  performance_biased_neg  \n",
       "0                 0.659302                0.623265  \n",
       "1                 0.578054                0.545134  \n",
       "2                 0.913797                0.809174  \n",
       "3                 0.753744                0.510867  \n",
       "4                 0.408307                0.572183  \n",
       "5                 0.632043                0.907886  \n",
       "6                 0.074163                0.059499  \n",
       "7                 0.084505                0.288727  \n",
       "8                 0.065228                0.053629  \n",
       "9                 0.069958                0.276613  \n",
       "10                0.000000                0.000000  \n",
       "11                0.000000                0.000000  \n",
       "12                0.024233                0.016768  \n",
       "13                0.044340                0.033545  \n",
       "14                0.953150                0.926067  \n",
       "15                0.955660                0.941483  \n",
       "16                0.854592                0.734949  \n",
       "17                0.690692                0.468713  \n",
       "18                0.856770                0.736896  \n",
       "19                0.694788                0.470708  \n",
       "20                0.949653                0.849379  \n",
       "21                0.818533                0.574526  \n",
       "22                1.000000                1.000000  \n",
       "23                1.000000                1.000000  \n",
       "24                0.993128                0.993128  \n",
       "25                0.974229                0.974229  \n",
       "26                0.150357                0.150357  \n",
       "27                0.089565                0.089565  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>phase</th>\n",
       "      <th>group</th>\n",
       "      <th>performance</th>\n",
       "      <th>performance_ipcw</th>\n",
       "      <th>performance_biased_obs</th>\n",
       "      <th>performance_biased_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.048192</td>\n",
       "      <td>-0.052012</td>\n",
       "      <td>-0.056867</td>\n",
       "      <td>-0.057959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023296</td>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.020172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auprc_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.112055</td>\n",
       "      <td>-0.114023</td>\n",
       "      <td>-0.102756</td>\n",
       "      <td>-0.172706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>auprc_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081074</td>\n",
       "      <td>0.081373</td>\n",
       "      <td>0.057297</td>\n",
       "      <td>0.125601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loss_bce_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.122413</td>\n",
       "      <td>0.124568</td>\n",
       "      <td>0.123951</td>\n",
       "      <td>0.166173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loss_bce_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.124886</td>\n",
       "      <td>-0.125651</td>\n",
       "      <td>-0.099784</td>\n",
       "      <td>-0.169530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ace_rmse_logistic_log_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077132</td>\n",
       "      <td>0.078538</td>\n",
       "      <td>0.043091</td>\n",
       "      <td>0.119141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ace_rmse_logistic_log_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.021114</td>\n",
       "      <td>-0.018638</td>\n",
       "      <td>0.032749</td>\n",
       "      <td>-0.110087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ace_abs_logistic_log_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.073715</td>\n",
       "      <td>0.074440</td>\n",
       "      <td>0.034361</td>\n",
       "      <td>0.110378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ace_abs_logistic_log_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.016438</td>\n",
       "      <td>-0.012233</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>-0.112606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>specificity_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>specificity_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>specificity_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.005026</td>\n",
       "      <td>0.007413</td>\n",
       "      <td>0.005509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>specificity_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012924</td>\n",
       "      <td>-0.010280</td>\n",
       "      <td>-0.012694</td>\n",
       "      <td>-0.011267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>specificity_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.005063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>specificity_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.009223</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>-0.001585</td>\n",
       "      <td>-0.010353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>precision_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.094998</td>\n",
       "      <td>-0.095280</td>\n",
       "      <td>-0.090802</td>\n",
       "      <td>-0.131787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>precision_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.096917</td>\n",
       "      <td>0.096108</td>\n",
       "      <td>0.073098</td>\n",
       "      <td>0.134449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>precision_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.095134</td>\n",
       "      <td>-0.095966</td>\n",
       "      <td>-0.090642</td>\n",
       "      <td>-0.133148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>precision_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095057</td>\n",
       "      <td>0.094778</td>\n",
       "      <td>0.071340</td>\n",
       "      <td>0.133039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>precision_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.095595</td>\n",
       "      <td>-0.106803</td>\n",
       "      <td>-0.090449</td>\n",
       "      <td>-0.174734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>precision_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054774</td>\n",
       "      <td>0.062266</td>\n",
       "      <td>0.040671</td>\n",
       "      <td>0.100119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>recall_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>recall_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>recall_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.009552</td>\n",
       "      <td>-0.010613</td>\n",
       "      <td>-0.011450</td>\n",
       "      <td>-0.011450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>recall_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.007449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>recall_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.031756</td>\n",
       "      <td>-0.031348</td>\n",
       "      <td>-0.036830</td>\n",
       "      <td>-0.036830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>recall_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.024184</td>\n",
       "      <td>0.023963</td>\n",
       "      <td>0.023963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>emd_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021411</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.026189</td>\n",
       "      <td>0.021411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>emd_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>0.022246</td>\n",
       "      <td>0.021083</td>\n",
       "      <td>0.021843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>emd_0_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>0.006929</td>\n",
       "      <td>0.005986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>emd_0_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>0.011286</td>\n",
       "      <td>0.011865</td>\n",
       "      <td>0.012241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>emd_1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023597</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.027938</td>\n",
       "      <td>0.027938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>emd_1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.019235</td>\n",
       "      <td>0.018178</td>\n",
       "      <td>0.018178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>xauc_1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560967</td>\n",
       "      <td>0.557317</td>\n",
       "      <td>0.563254</td>\n",
       "      <td>0.531303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>xauc_1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.670599</td>\n",
       "      <td>0.681549</td>\n",
       "      <td>0.649802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>xauc_0_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.634709</td>\n",
       "      <td>0.632490</td>\n",
       "      <td>0.648624</td>\n",
       "      <td>0.616407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>xauc_0_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.596690</td>\n",
       "      <td>0.598305</td>\n",
       "      <td>0.611453</td>\n",
       "      <td>0.575867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xauc_1</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533928</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.537911</td>\n",
       "      <td>0.503019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>xauc_1</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.681929</td>\n",
       "      <td>0.681275</td>\n",
       "      <td>0.694540</td>\n",
       "      <td>0.662779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>xauc_0</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.681929</td>\n",
       "      <td>0.681275</td>\n",
       "      <td>0.694540</td>\n",
       "      <td>0.662779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>xauc_0</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.533928</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.537911</td>\n",
       "      <td>0.503019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       metric phase  group  performance  performance_ipcw  \\\n",
       "0                     auc_ova  test      1    -0.048192         -0.052012   \n",
       "1                     auc_ova  test      0     0.023296          0.027497   \n",
       "2                   auprc_ova  test      1    -0.112055         -0.114023   \n",
       "3                   auprc_ova  test      0     0.081074          0.081373   \n",
       "4                loss_bce_ova  test      1     0.122413          0.124568   \n",
       "5                loss_bce_ova  test      0    -0.124886         -0.125651   \n",
       "6   ace_rmse_logistic_log_ova  test      1     0.077132          0.078538   \n",
       "7   ace_rmse_logistic_log_ova  test      0    -0.021114         -0.018638   \n",
       "8    ace_abs_logistic_log_ova  test      1     0.073715          0.074440   \n",
       "9    ace_abs_logistic_log_ova  test      0    -0.016438         -0.012233   \n",
       "10        specificity_0.1_ova  test      1     0.000000          0.000000   \n",
       "11        specificity_0.1_ova  test      0     0.000000          0.000000   \n",
       "12        specificity_0.5_ova  test      1     0.006273          0.005026   \n",
       "13        specificity_0.5_ova  test      0    -0.012924         -0.010280   \n",
       "14        specificity_0.9_ova  test      1     0.004477          0.000695   \n",
       "15        specificity_0.9_ova  test      0    -0.009223         -0.001421   \n",
       "16          precision_0.1_ova  test      1    -0.094998         -0.095280   \n",
       "17          precision_0.1_ova  test      0     0.096917          0.096108   \n",
       "18          precision_0.5_ova  test      1    -0.095134         -0.095966   \n",
       "19          precision_0.5_ova  test      0     0.095057          0.094778   \n",
       "20          precision_0.9_ova  test      1    -0.095595         -0.106803   \n",
       "21          precision_0.9_ova  test      0     0.054774          0.062266   \n",
       "22             recall_0.1_ova  test      1     0.000000          0.000000   \n",
       "23             recall_0.1_ova  test      0     0.000000          0.000000   \n",
       "24             recall_0.5_ova  test      1    -0.009552         -0.010613   \n",
       "25             recall_0.5_ova  test      0     0.007441          0.008188   \n",
       "26             recall_0.9_ova  test      1    -0.031756         -0.031348   \n",
       "27             recall_0.9_ova  test      0     0.024738          0.024184   \n",
       "28                    emd_ova  test      1     0.021411          0.022054   \n",
       "29                    emd_ova  test      0     0.021843          0.022246   \n",
       "30                  emd_0_ova  test      1     0.005998          0.005517   \n",
       "31                  emd_0_ova  test      0     0.012357          0.011286   \n",
       "32                  emd_1_ova  test      1     0.023597          0.024933   \n",
       "33                  emd_1_ova  test      0     0.018382          0.019235   \n",
       "34                 xauc_1_ova  test      1     0.560967          0.557317   \n",
       "35                 xauc_1_ova  test      0     0.670051          0.670599   \n",
       "36                 xauc_0_ova  test      1     0.634709          0.632490   \n",
       "37                 xauc_0_ova  test      0     0.596690          0.598305   \n",
       "38                     xauc_1  test      1     0.533928          0.532900   \n",
       "39                     xauc_1  test      0     0.681929          0.681275   \n",
       "40                     xauc_0  test      1     0.681929          0.681275   \n",
       "41                     xauc_0  test      0     0.533928          0.532900   \n",
       "\n",
       "    performance_biased_obs  performance_biased_neg  \n",
       "0                -0.056867               -0.057959  \n",
       "1                 0.024382                0.020172  \n",
       "2                -0.102756               -0.172706  \n",
       "3                 0.057297                0.125601  \n",
       "4                 0.123951                0.166173  \n",
       "5                -0.099784               -0.169530  \n",
       "6                 0.043091                0.119141  \n",
       "7                 0.032749               -0.110087  \n",
       "8                 0.034361                0.110378  \n",
       "9                 0.029630               -0.112606  \n",
       "10                0.000000                0.000000  \n",
       "11                0.000000                0.000000  \n",
       "12                0.007413                0.005509  \n",
       "13               -0.012694               -0.011267  \n",
       "14                0.000925                0.005063  \n",
       "15               -0.001585               -0.010353  \n",
       "16               -0.090802               -0.131787  \n",
       "17                0.073098                0.134449  \n",
       "18               -0.090642               -0.133148  \n",
       "19                0.071340                0.133039  \n",
       "20               -0.090449               -0.174734  \n",
       "21                0.040671                0.100119  \n",
       "22                0.000000                0.000000  \n",
       "23                0.000000                0.000000  \n",
       "24               -0.011450               -0.011450  \n",
       "25                0.007449                0.007449  \n",
       "26               -0.036830               -0.036830  \n",
       "27                0.023963                0.023963  \n",
       "28                0.026189                0.021411  \n",
       "29                0.021083                0.021843  \n",
       "30                0.006929                0.005986  \n",
       "31                0.011865                0.012241  \n",
       "32                0.027938                0.027938  \n",
       "33                0.018178                0.018178  \n",
       "34                0.563254                0.531303  \n",
       "35                0.681549                0.649802  \n",
       "36                0.648624                0.616407  \n",
       "37                0.611453                0.575867  \n",
       "38                0.537911                0.503019  \n",
       "39                0.694540                0.662779  \n",
       "40                0.694540                0.662779  \n",
       "41                0.537911                0.503019  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation of unweighted model\n",
    "loader_generator = ArrayLoaderGenerator(\n",
    "    features=np.concatenate((features.astype(np.float32), df.group.values.reshape(-1, 1).astype(np.float32)), axis=1), \n",
    "    cohort=df, \n",
    "    fold_id=fold_id,\n",
    "    eval_key='test', \n",
    "    row_id_col='row_id', \n",
    "    label_col='observed_labels',\n",
    "    **config_dict\n",
    ")\n",
    "\n",
    "loaders_predict = loader_generator.init_loaders_predict()\n",
    "\n",
    "predict_dict = model.predict(loaders_predict, phases=['test'])\n",
    "\n",
    "output_df_eval, result_df_eval = (\n",
    "    predict_dict[\"outputs\"],\n",
    "    predict_dict[\"performance\"],\n",
    ")\n",
    "\n",
    "output_df_eval = output_df_eval.drop(columns=['labels']).merge(\n",
    "    labels_df_dict['test'],\n",
    ").assign(labels = lambda x: x.labels.astype(np.long))\n",
    "\n",
    "display(temp_evaluate(df=output_df_eval, strata_vars=None))\n",
    "\n",
    "display(temp_evaluate(df=output_df_eval, strata_vars=['group']))\n",
    "\n",
    "display(temp_evaluate_fair(df=output_df_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "cuda\n",
      "Epoch 0/19\n",
      "----------\n",
      "Phase: train:\n",
      "               metric  group  performance\n",
      "0                 auc    0.0     0.528574\n",
      "1                 auc    1.0     0.573145\n",
      "2               auprc    0.0     0.858299\n",
      "3               auprc    1.0     0.728893\n",
      "4               brier    0.0     0.255799\n",
      "5               brier    1.0     0.256318\n",
      "6            loss_bce    0.0     0.711067\n",
      "7            loss_bce    1.0     0.713486\n",
      "8   specificity_0.075    0.0     0.000449\n",
      "9   specificity_0.075    1.0     0.001088\n",
      "10    specificity_0.2    0.0     0.033693\n",
      "11    specificity_0.2    1.0     0.055223\n",
      "12    precision_0.075    0.0     0.844199\n",
      "13    precision_0.075    1.0     0.675015\n",
      "14      precision_0.2    0.0     0.844457\n",
      "15      precision_0.2    1.0     0.680350\n",
      "16       recall_0.075    0.0     0.999503\n",
      "17       recall_0.075    1.0     0.998821\n",
      "18         recall_0.2    0.0     0.968164\n",
      "19         recall_0.2    1.0     0.968046\n",
      "0                loss    NaN     0.703229\n",
      "1          supervised    NaN     0.712136\n",
      "Phase: val:\n",
      "               metric  group  performance\n",
      "0                 auc    0.0     0.543640\n",
      "1                 auc    1.0     0.558298\n",
      "2               auprc    0.0     0.861975\n",
      "3               auprc    1.0     0.734271\n",
      "4               brier    0.0     0.245737\n",
      "5               brier    1.0     0.256668\n",
      "6            loss_bce    0.0     0.688442\n",
      "7            loss_bce    1.0     0.712829\n",
      "8   specificity_0.075    0.0     0.000000\n",
      "9   specificity_0.075    1.0     0.000000\n",
      "10    specificity_0.2    0.0     0.042553\n",
      "11    specificity_0.2    1.0     0.051493\n",
      "12    precision_0.075    0.0     0.843213\n",
      "13    precision_0.075    1.0     0.684740\n",
      "14      precision_0.2    0.0     0.845443\n",
      "15      precision_0.2    1.0     0.690212\n",
      "16       recall_0.075    0.0     0.999696\n",
      "17       recall_0.075    1.0     1.000000\n",
      "18         recall_0.2    0.0     0.973532\n",
      "19         recall_0.2    1.0     0.972973\n",
      "0                loss    NaN     0.690697\n",
      "1          supervised    NaN     0.699206\n",
      "Epoch 10/19\n",
      "----------\n",
      "Phase: train:\n",
      "               metric  group  performance\n",
      "0                 auc    0.0     0.589384\n",
      "1                 auc    1.0     0.581996\n",
      "2               auprc    0.0     0.879935\n",
      "3               auprc    1.0     0.734701\n",
      "4               brier    0.0     0.164786\n",
      "5               brier    1.0     0.223926\n",
      "6            loss_bce    0.0     0.507211\n",
      "7            loss_bce    1.0     0.645692\n",
      "8   specificity_0.075    0.0     0.000000\n",
      "9   specificity_0.075    1.0     0.000000\n",
      "10    specificity_0.2    0.0     0.001793\n",
      "11    specificity_0.2    1.0     0.000816\n",
      "12    precision_0.075    0.0     0.844193\n",
      "13    precision_0.075    1.0     0.673965\n",
      "14      precision_0.2    0.0     0.844320\n",
      "15      precision_0.2    1.0     0.673942\n",
      "16       recall_0.075    0.0     1.000000\n",
      "17       recall_0.075    1.0     1.000000\n",
      "18         recall_0.2    0.0     0.999173\n",
      "19         recall_0.2    1.0     0.999079\n",
      "0                loss    NaN     0.564660\n",
      "1          supervised    NaN     0.568235\n",
      "Phase: val:\n",
      "               metric  group  performance\n",
      "0                 auc    0.0     0.599159\n",
      "1                 auc    1.0     0.574393\n",
      "2               auprc    0.0     0.884821\n",
      "3               auprc    1.0     0.747108\n",
      "4               brier    0.0     0.161788\n",
      "5               brier    1.0     0.221109\n",
      "6            loss_bce    0.0     0.499356\n",
      "7            loss_bce    1.0     0.637956\n",
      "8   specificity_0.075    0.0     0.000000\n",
      "9   specificity_0.075    1.0     0.000000\n",
      "10    specificity_0.2    0.0     0.001637\n",
      "11    specificity_0.2    1.0     0.003090\n",
      "12    precision_0.075    0.0     0.843253\n",
      "13    precision_0.075    1.0     0.684740\n",
      "14      precision_0.2    0.0     0.843429\n",
      "15      precision_0.2    1.0     0.685408\n",
      "16       recall_0.075    0.0     1.000000\n",
      "17       recall_0.075    1.0     1.000000\n",
      "18         recall_0.2    0.0     0.999696\n",
      "19         recall_0.2    1.0     1.000000\n",
      "0                loss    NaN     0.557020\n",
      "1          supervised    NaN     0.560532\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6ef8874f14c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m result_dict = model.train(\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/prediction_utils/prediction_utils/pytorch_utils/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loaders, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 \u001b[0mepoch_performance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"print_every\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Phase: {}:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/prediction_utils/prediction_utils/pytorch_utils/metric_logging.py\u001b[0m in \u001b[0;36mcompute_metrics_epoch\u001b[0;34m(self, phase)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         metric_df = self.evaluator.evaluate(\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mweight_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weights\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_evaluation\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/prediction_utils/prediction_utils/pytorch_utils/metrics.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, df, strata_vars, result_name, weight_var, label_var, pred_prob_var)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             result_df = df_dict_concat(\n\u001b[0;32m--> 515\u001b[0;31m                 {\n\u001b[0m\u001b[1;32m    516\u001b[0m                     \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrata_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                     .apply(\n",
      "\u001b[0;32m~/projects/prediction_utils/prediction_utils/pytorch_utils/metrics.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    514\u001b[0m             result_df = df_dict_concat(\n\u001b[1;32m    515\u001b[0m                 {\n\u001b[0;32m--> 516\u001b[0;31m                     \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrata_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                     .apply(\n\u001b[1;32m    518\u001b[0m                         lambda x: metric_func(\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \"\"\"\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFrameOrSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mgroup_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_group_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mresult_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m_get_splitter\u001b[0;34m(self, data, axis)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0m__finalize__\u001b[0m \u001b[0mhas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msubsetted\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \"\"\"\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mcomp_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mgroup_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgroup_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mcomp_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_group_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_compressed_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mngroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_group_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m_get_compressed_codes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_compressed_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mall_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_codes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mgroup_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_group_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxnull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mcodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mping\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mping\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mcodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36m_make_codes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                 \u001b[0mna_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             codes, uniques = algorithms.factorize(\n\u001b[0m\u001b[1;32m    624\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n",
      "\u001b[0;32m/local-scratch/nigam/envs/prediction_utils/lib/python3.9/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    729\u001b[0m         )\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m     \u001b[0mcode_is_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdropna\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcode_is_na\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;31m# na_value is set based on the dtype of uniques, and compat set to False is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Unweighted Model - training for fairness\n",
    "loader_generator = ArrayLoaderGenerator(\n",
    "    features=np.concatenate((features.astype(np.float32), df.group.values.reshape(-1, 1).astype(np.float32)), axis=1), \n",
    "    cohort=df.query('binary_event_indicator == 1'), \n",
    "    label_col='observed_labels',\n",
    "    fold_id=fold_id,\n",
    "    eval_key='val', \n",
    "    row_id_col='row_id', \n",
    "    num_workers=0,\n",
    "    **config_dict\n",
    ")\n",
    "\n",
    "loaders = loader_generator.init_loaders()\n",
    "\n",
    "\n",
    "lagrangian_config={\n",
    "    'lr_lambda': 1e-1,\n",
    "    'constraint_slack': 0.01,\n",
    "    'multiplier_bound': 1,\n",
    "    'additive_update': False,\n",
    "    'use_exact_constraints': True,\n",
    "    'thresholds': [0.075, 0.2],\n",
    "    'constraint_metrics': ['tpr', 'fpr']\n",
    "}\n",
    "\n",
    "# thresholds = [0.075, 0.2]\n",
    "\n",
    "model_class = group_lagrangian_model(\n",
    "    'multi'\n",
    ")\n",
    "\n",
    "model = model_class(\n",
    "    **config_dict, \n",
    "    **lagrangian_config,\n",
    "    logging_thresholds=lagrangian_config.get('thresholds'), \n",
    ")\n",
    "\n",
    "result_dict = model.train(\n",
    "    loaders,\n",
    "    lr=1e-5,\n",
    "    num_epochs=20,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=10,\n",
    "    print_debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_surrogate_fn()(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Evaluating on phase: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-57821425bbb1>:24: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ).assign(labels = lambda x: x.labels.astype(np.long))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>performance</th>\n",
       "      <th>performance_ipcw</th>\n",
       "      <th>performance_biased_obs</th>\n",
       "      <th>performance_biased_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.568866</td>\n",
       "      <td>0.563097</td>\n",
       "      <td>0.574014</td>\n",
       "      <td>0.551908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auprc</td>\n",
       "      <td>0.756919</td>\n",
       "      <td>0.757462</td>\n",
       "      <td>0.820254</td>\n",
       "      <td>0.638077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loss_bce</td>\n",
       "      <td>0.614556</td>\n",
       "      <td>0.614901</td>\n",
       "      <td>0.562608</td>\n",
       "      <td>0.711465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ace_rmse_logistic_log</td>\n",
       "      <td>0.100523</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>0.142514</td>\n",
       "      <td>0.123773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ace_abs_logistic_log</td>\n",
       "      <td>0.081527</td>\n",
       "      <td>0.084911</td>\n",
       "      <td>0.115682</td>\n",
       "      <td>0.103484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>specificity_0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>specificity_0.5</td>\n",
       "      <td>0.159537</td>\n",
       "      <td>0.155032</td>\n",
       "      <td>0.162597</td>\n",
       "      <td>0.142678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>specificity_0.9</td>\n",
       "      <td>0.980715</td>\n",
       "      <td>0.982175</td>\n",
       "      <td>0.983919</td>\n",
       "      <td>0.979474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>precision_0.1</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>0.717697</td>\n",
       "      <td>0.781494</td>\n",
       "      <td>0.600500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>precision_0.5</td>\n",
       "      <td>0.727676</td>\n",
       "      <td>0.729151</td>\n",
       "      <td>0.792717</td>\n",
       "      <td>0.610884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>precision_0.9</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.789630</td>\n",
       "      <td>0.855615</td>\n",
       "      <td>0.661157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>recall_0.1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>recall_0.5</td>\n",
       "      <td>0.896055</td>\n",
       "      <td>0.894755</td>\n",
       "      <td>0.895420</td>\n",
       "      <td>0.895420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>recall_0.9</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>0.026317</td>\n",
       "      <td>0.026644</td>\n",
       "      <td>0.026644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   metric  performance  performance_ipcw  \\\n",
       "0                     auc     0.568866          0.563097   \n",
       "1                   auprc     0.756919          0.757462   \n",
       "2                loss_bce     0.614556          0.614901   \n",
       "3   ace_rmse_logistic_log     0.100523          0.105608   \n",
       "4    ace_abs_logistic_log     0.081527          0.084911   \n",
       "5         specificity_0.1     0.000000          0.000000   \n",
       "6         specificity_0.5     0.159537          0.155032   \n",
       "7         specificity_0.9     0.980715          0.982175   \n",
       "8           precision_0.1     0.714800          0.717697   \n",
       "9           precision_0.5     0.727676          0.729151   \n",
       "10          precision_0.9     0.772727          0.789630   \n",
       "11             recall_0.1     1.000000          1.000000   \n",
       "12             recall_0.5     0.896055          0.894755   \n",
       "13             recall_0.9     0.026161          0.026317   \n",
       "\n",
       "    performance_biased_obs  performance_biased_neg  \n",
       "0                 0.574014                0.551908  \n",
       "1                 0.820254                0.638077  \n",
       "2                 0.562608                0.711465  \n",
       "3                 0.142514                0.123773  \n",
       "4                 0.115682                0.103484  \n",
       "5                 0.000000                0.000000  \n",
       "6                 0.162597                0.142678  \n",
       "7                 0.983919                0.979474  \n",
       "8                 0.781494                0.600500  \n",
       "9                 0.792717                0.610884  \n",
       "10                0.855615                0.661157  \n",
       "11                1.000000                1.000000  \n",
       "12                0.895420                0.895420  \n",
       "13                0.026644                0.026644  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>group</th>\n",
       "      <th>performance</th>\n",
       "      <th>performance_ipcw</th>\n",
       "      <th>performance_biased_obs</th>\n",
       "      <th>performance_biased_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.581981</td>\n",
       "      <td>0.586873</td>\n",
       "      <td>0.592107</td>\n",
       "      <td>0.578773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566123</td>\n",
       "      <td>0.553334</td>\n",
       "      <td>0.563620</td>\n",
       "      <td>0.539201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auprc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850110</td>\n",
       "      <td>0.855302</td>\n",
       "      <td>0.889768</td>\n",
       "      <td>0.779831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>auprc</td>\n",
       "      <td>1</td>\n",
       "      <td>0.667155</td>\n",
       "      <td>0.662820</td>\n",
       "      <td>0.734163</td>\n",
       "      <td>0.501460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loss_bce</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538362</td>\n",
       "      <td>0.534493</td>\n",
       "      <td>0.503177</td>\n",
       "      <td>0.595323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loss_bce</td>\n",
       "      <td>1</td>\n",
       "      <td>0.689241</td>\n",
       "      <td>0.694617</td>\n",
       "      <td>0.636432</td>\n",
       "      <td>0.825306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ace_rmse_logistic_log</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166284</td>\n",
       "      <td>0.166406</td>\n",
       "      <td>0.201516</td>\n",
       "      <td>0.106406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ace_rmse_logistic_log</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106028</td>\n",
       "      <td>0.114823</td>\n",
       "      <td>0.099568</td>\n",
       "      <td>0.233933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ace_abs_logistic_log</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140305</td>\n",
       "      <td>0.141002</td>\n",
       "      <td>0.176404</td>\n",
       "      <td>0.086189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ace_abs_logistic_log</td>\n",
       "      <td>1</td>\n",
       "      <td>0.088231</td>\n",
       "      <td>0.096233</td>\n",
       "      <td>0.081879</td>\n",
       "      <td>0.209766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>specificity_0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>specificity_0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>specificity_0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.171674</td>\n",
       "      <td>0.172985</td>\n",
       "      <td>0.176090</td>\n",
       "      <td>0.160823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>specificity_0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153646</td>\n",
       "      <td>0.146255</td>\n",
       "      <td>0.154717</td>\n",
       "      <td>0.133805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>specificity_0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.986052</td>\n",
       "      <td>0.990260</td>\n",
       "      <td>0.990307</td>\n",
       "      <td>0.982470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>specificity_0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978125</td>\n",
       "      <td>0.978223</td>\n",
       "      <td>0.980189</td>\n",
       "      <td>0.978010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>precision_0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811717</td>\n",
       "      <td>0.813805</td>\n",
       "      <td>0.854592</td>\n",
       "      <td>0.734949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>precision_0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619802</td>\n",
       "      <td>0.622417</td>\n",
       "      <td>0.690692</td>\n",
       "      <td>0.468713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>precision_0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.823422</td>\n",
       "      <td>0.825994</td>\n",
       "      <td>0.865115</td>\n",
       "      <td>0.748170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>precision_0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633183</td>\n",
       "      <td>0.632216</td>\n",
       "      <td>0.701532</td>\n",
       "      <td>0.475395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>precision_0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.884956</td>\n",
       "      <td>0.915193</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.796460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>precision_0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.688923</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.542636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>recall_0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>recall_0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>recall_0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895968</td>\n",
       "      <td>0.898199</td>\n",
       "      <td>0.899120</td>\n",
       "      <td>0.899120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>recall_0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896166</td>\n",
       "      <td>0.890290</td>\n",
       "      <td>0.889734</td>\n",
       "      <td>0.889734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>recall_0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024888</td>\n",
       "      <td>0.024049</td>\n",
       "      <td>0.024739</td>\n",
       "      <td>0.024739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>recall_0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027796</td>\n",
       "      <td>0.029257</td>\n",
       "      <td>0.029573</td>\n",
       "      <td>0.029573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   metric  group  performance  performance_ipcw  \\\n",
       "0                     auc      0     0.581981          0.586873   \n",
       "1                     auc      1     0.566123          0.553334   \n",
       "2                   auprc      0     0.850110          0.855302   \n",
       "3                   auprc      1     0.667155          0.662820   \n",
       "4                loss_bce      0     0.538362          0.534493   \n",
       "5                loss_bce      1     0.689241          0.694617   \n",
       "6   ace_rmse_logistic_log      0     0.166284          0.166406   \n",
       "7   ace_rmse_logistic_log      1     0.106028          0.114823   \n",
       "8    ace_abs_logistic_log      0     0.140305          0.141002   \n",
       "9    ace_abs_logistic_log      1     0.088231          0.096233   \n",
       "10        specificity_0.1      0     0.000000          0.000000   \n",
       "11        specificity_0.1      1     0.000000          0.000000   \n",
       "12        specificity_0.5      0     0.171674          0.172985   \n",
       "13        specificity_0.5      1     0.153646          0.146255   \n",
       "14        specificity_0.9      0     0.986052          0.990260   \n",
       "15        specificity_0.9      1     0.978125          0.978223   \n",
       "16          precision_0.1      0     0.811717          0.813805   \n",
       "17          precision_0.1      1     0.619802          0.622417   \n",
       "18          precision_0.5      0     0.823422          0.825994   \n",
       "19          precision_0.5      1     0.633183          0.632216   \n",
       "20          precision_0.9      0     0.884956          0.915193   \n",
       "21          precision_0.9      1     0.674419          0.688923   \n",
       "22             recall_0.1      0     1.000000          1.000000   \n",
       "23             recall_0.1      1     1.000000          1.000000   \n",
       "24             recall_0.5      0     0.895968          0.898199   \n",
       "25             recall_0.5      1     0.896166          0.890290   \n",
       "26             recall_0.9      0     0.024888          0.024049   \n",
       "27             recall_0.9      1     0.027796          0.029257   \n",
       "\n",
       "    performance_biased_obs  performance_biased_neg  \n",
       "0                 0.592107                0.578773  \n",
       "1                 0.563620                0.539201  \n",
       "2                 0.889768                0.779831  \n",
       "3                 0.734163                0.501460  \n",
       "4                 0.503177                0.595323  \n",
       "5                 0.636432                0.825306  \n",
       "6                 0.201516                0.106406  \n",
       "7                 0.099568                0.233933  \n",
       "8                 0.176404                0.086189  \n",
       "9                 0.081879                0.209766  \n",
       "10                0.000000                0.000000  \n",
       "11                0.000000                0.000000  \n",
       "12                0.176090                0.160823  \n",
       "13                0.154717                0.133805  \n",
       "14                0.990307                0.982470  \n",
       "15                0.980189                0.978010  \n",
       "16                0.854592                0.734949  \n",
       "17                0.690692                0.468713  \n",
       "18                0.865115                0.748170  \n",
       "19                0.701532                0.475395  \n",
       "20                0.937500                0.796460  \n",
       "21                0.769231                0.542636  \n",
       "22                1.000000                1.000000  \n",
       "23                1.000000                1.000000  \n",
       "24                0.899120                0.899120  \n",
       "25                0.889734                0.889734  \n",
       "26                0.024739                0.024739  \n",
       "27                0.029573                0.029573  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>phase</th>\n",
       "      <th>group</th>\n",
       "      <th>performance</th>\n",
       "      <th>performance_ipcw</th>\n",
       "      <th>performance_biased_obs</th>\n",
       "      <th>performance_biased_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.002742</td>\n",
       "      <td>-0.009763</td>\n",
       "      <td>-0.010394</td>\n",
       "      <td>-0.012707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013116</td>\n",
       "      <td>0.023776</td>\n",
       "      <td>0.018093</td>\n",
       "      <td>0.026865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auprc_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.089764</td>\n",
       "      <td>-0.094643</td>\n",
       "      <td>-0.086090</td>\n",
       "      <td>-0.136617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>auprc_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093191</td>\n",
       "      <td>0.097840</td>\n",
       "      <td>0.069514</td>\n",
       "      <td>0.141755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loss_bce_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074685</td>\n",
       "      <td>0.079715</td>\n",
       "      <td>0.073825</td>\n",
       "      <td>0.113842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loss_bce_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.076194</td>\n",
       "      <td>-0.080408</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.116142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ace_rmse_logistic_log_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005505</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>-0.042946</td>\n",
       "      <td>0.110160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ace_rmse_logistic_log_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065761</td>\n",
       "      <td>0.060798</td>\n",
       "      <td>0.059001</td>\n",
       "      <td>-0.017367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ace_abs_logistic_log_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006705</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>-0.033804</td>\n",
       "      <td>0.106281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ace_abs_logistic_log_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058778</td>\n",
       "      <td>0.056091</td>\n",
       "      <td>0.060721</td>\n",
       "      <td>-0.017295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>specificity_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>specificity_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>specificity_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>-0.008777</td>\n",
       "      <td>-0.007880</td>\n",
       "      <td>-0.008873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>specificity_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.013494</td>\n",
       "      <td>0.018145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>specificity_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>-0.003952</td>\n",
       "      <td>-0.003730</td>\n",
       "      <td>-0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>specificity_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005336</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>0.006388</td>\n",
       "      <td>0.002995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>precision_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.094998</td>\n",
       "      <td>-0.095280</td>\n",
       "      <td>-0.090802</td>\n",
       "      <td>-0.131787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>precision_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.096917</td>\n",
       "      <td>0.096108</td>\n",
       "      <td>0.073098</td>\n",
       "      <td>0.134449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>precision_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.094493</td>\n",
       "      <td>-0.096935</td>\n",
       "      <td>-0.091185</td>\n",
       "      <td>-0.135489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>precision_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095746</td>\n",
       "      <td>0.096843</td>\n",
       "      <td>0.072398</td>\n",
       "      <td>0.137286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>precision_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.098309</td>\n",
       "      <td>-0.100707</td>\n",
       "      <td>-0.086384</td>\n",
       "      <td>-0.118521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>precision_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112228</td>\n",
       "      <td>0.125564</td>\n",
       "      <td>0.081885</td>\n",
       "      <td>0.135303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>recall_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>recall_0.1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>recall_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>-0.005687</td>\n",
       "      <td>-0.005687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>recall_0.5_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>recall_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>recall_0.9_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001273</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.001906</td>\n",
       "      <td>-0.001906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>emd_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>0.001464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>emd_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.001494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>emd_0_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.006345</td>\n",
       "      <td>0.005537</td>\n",
       "      <td>0.006105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>emd_0_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>0.009483</td>\n",
       "      <td>0.012484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>emd_1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.002128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>emd_1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>xauc_1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.574548</td>\n",
       "      <td>0.565742</td>\n",
       "      <td>0.574067</td>\n",
       "      <td>0.552225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>xauc_1_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.564439</td>\n",
       "      <td>0.561056</td>\n",
       "      <td>0.573980</td>\n",
       "      <td>0.551702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>xauc_0_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560390</td>\n",
       "      <td>0.550568</td>\n",
       "      <td>0.563484</td>\n",
       "      <td>0.538755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>xauc_0_ova</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.586326</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.592048</td>\n",
       "      <td>0.578807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xauc_1</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.591903</td>\n",
       "      <td>0.591122</td>\n",
       "      <td>0.591956</td>\n",
       "      <td>0.578859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>xauc_1</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.555924</td>\n",
       "      <td>0.548435</td>\n",
       "      <td>0.563395</td>\n",
       "      <td>0.538464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>xauc_0</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555924</td>\n",
       "      <td>0.548435</td>\n",
       "      <td>0.563395</td>\n",
       "      <td>0.538464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>xauc_0</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.591903</td>\n",
       "      <td>0.591122</td>\n",
       "      <td>0.591956</td>\n",
       "      <td>0.578859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       metric phase  group  performance  performance_ipcw  \\\n",
       "0                     auc_ova  test      1    -0.002742         -0.009763   \n",
       "1                     auc_ova  test      0     0.013116          0.023776   \n",
       "2                   auprc_ova  test      1    -0.089764         -0.094643   \n",
       "3                   auprc_ova  test      0     0.093191          0.097840   \n",
       "4                loss_bce_ova  test      1     0.074685          0.079715   \n",
       "5                loss_bce_ova  test      0    -0.076194         -0.080408   \n",
       "6   ace_rmse_logistic_log_ova  test      1     0.005505          0.009214   \n",
       "7   ace_rmse_logistic_log_ova  test      0     0.065761          0.060798   \n",
       "8    ace_abs_logistic_log_ova  test      1     0.006705          0.011321   \n",
       "9    ace_abs_logistic_log_ova  test      0     0.058778          0.056091   \n",
       "10        specificity_0.1_ova  test      1     0.000000          0.000000   \n",
       "11        specificity_0.1_ova  test      0     0.000000          0.000000   \n",
       "12        specificity_0.5_ova  test      1    -0.005891         -0.008777   \n",
       "13        specificity_0.5_ova  test      0     0.012137          0.017953   \n",
       "14        specificity_0.9_ova  test      1    -0.002590         -0.003952   \n",
       "15        specificity_0.9_ova  test      0     0.005336          0.008085   \n",
       "16          precision_0.1_ova  test      1    -0.094998         -0.095280   \n",
       "17          precision_0.1_ova  test      0     0.096917          0.096108   \n",
       "18          precision_0.5_ova  test      1    -0.094493         -0.096935   \n",
       "19          precision_0.5_ova  test      0     0.095746          0.096843   \n",
       "20          precision_0.9_ova  test      1    -0.098309         -0.100707   \n",
       "21          precision_0.9_ova  test      0     0.112228          0.125564   \n",
       "22             recall_0.1_ova  test      1     0.000000          0.000000   \n",
       "23             recall_0.1_ova  test      0     0.000000          0.000000   \n",
       "24             recall_0.5_ova  test      1     0.000111         -0.004465   \n",
       "25             recall_0.5_ova  test      0    -0.000087          0.003444   \n",
       "26             recall_0.9_ova  test      1     0.001634          0.002940   \n",
       "27             recall_0.9_ova  test      0    -0.001273         -0.002268   \n",
       "28                    emd_ova  test      1     0.001464          0.001667   \n",
       "29                    emd_ova  test      0     0.001494          0.001682   \n",
       "30                  emd_0_ova  test      1     0.004162          0.006345   \n",
       "31                  emd_0_ova  test      0     0.008575          0.012979   \n",
       "32                  emd_1_ova  test      1     0.002845          0.002292   \n",
       "33                  emd_1_ova  test      0     0.002216          0.001768   \n",
       "34                 xauc_1_ova  test      1     0.574548          0.565742   \n",
       "35                 xauc_1_ova  test      0     0.564439          0.561056   \n",
       "36                 xauc_0_ova  test      1     0.560390          0.550568   \n",
       "37                 xauc_0_ova  test      0     0.586326          0.588723   \n",
       "38                     xauc_1  test      1     0.591903          0.591122   \n",
       "39                     xauc_1  test      0     0.555924          0.548435   \n",
       "40                     xauc_0  test      1     0.555924          0.548435   \n",
       "41                     xauc_0  test      0     0.591903          0.591122   \n",
       "\n",
       "    performance_biased_obs  performance_biased_neg  \n",
       "0                -0.010394               -0.012707  \n",
       "1                 0.018093                0.026865  \n",
       "2                -0.086090               -0.136617  \n",
       "3                 0.069514                0.141755  \n",
       "4                 0.073825                0.113842  \n",
       "5                -0.059431               -0.116142  \n",
       "6                -0.042946                0.110160  \n",
       "7                 0.059001               -0.017367  \n",
       "8                -0.033804                0.106281  \n",
       "9                 0.060721               -0.017295  \n",
       "10                0.000000                0.000000  \n",
       "11                0.000000                0.000000  \n",
       "12               -0.007880               -0.008873  \n",
       "13                0.013494                0.018145  \n",
       "14               -0.003730               -0.001465  \n",
       "15                0.006388                0.002995  \n",
       "16               -0.090802               -0.131787  \n",
       "17                0.073098                0.134449  \n",
       "18               -0.091185               -0.135489  \n",
       "19                0.072398                0.137286  \n",
       "20               -0.086384               -0.118521  \n",
       "21                0.081885                0.135303  \n",
       "22                0.000000                0.000000  \n",
       "23                0.000000                0.000000  \n",
       "24               -0.005687               -0.005687  \n",
       "25                0.003700                0.003700  \n",
       "26                0.002929                0.002929  \n",
       "27               -0.001906               -0.001906  \n",
       "28                0.002314                0.001464  \n",
       "29                0.001863                0.001494  \n",
       "30                0.005537                0.006105  \n",
       "31                0.009483                0.012484  \n",
       "32                0.002128                0.002128  \n",
       "33                0.001384                0.001384  \n",
       "34                0.574067                0.552225  \n",
       "35                0.573980                0.551702  \n",
       "36                0.563484                0.538755  \n",
       "37                0.592048                0.578807  \n",
       "38                0.591956                0.578859  \n",
       "39                0.563395                0.538464  \n",
       "40                0.563395                0.538464  \n",
       "41                0.591956                0.578859  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation of unweighted fair model\n",
    "loader_generator = ArrayLoaderGenerator(\n",
    "    features=np.concatenate((features.astype(np.float32), df.group.values.reshape(-1, 1).astype(np.float32)), axis=1), \n",
    "    cohort=df, \n",
    "    fold_id=fold_id,\n",
    "    eval_key='test', \n",
    "    row_id_col='row_id', \n",
    "    label_col='true_labels',\n",
    "    include_group_in_dataset=True,\n",
    "    group_var_name='group'\n",
    ")\n",
    "\n",
    "loaders_predict = loader_generator.init_loaders_predict()\n",
    "\n",
    "predict_dict = model.predict(loaders_predict, phases=['test'])\n",
    "\n",
    "output_df_eval, result_df_eval = (\n",
    "    predict_dict[\"outputs\"],\n",
    "    predict_dict[\"performance\"],\n",
    ")\n",
    "\n",
    "output_df_eval = output_df_eval.drop(columns=['labels']).merge(\n",
    "    labels_df_dict['test'], \n",
    ").assign(labels = lambda x: x.labels.astype(np.long))\n",
    "\n",
    "display(temp_evaluate(df=output_df_eval, strata_vars=None))\n",
    "\n",
    "display(temp_evaluate(df=output_df_eval, strata_vars=['group']))\n",
    "\n",
    "display(temp_evaluate_fair(df=output_df_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "cuda\n",
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f9c8d7a1946a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mloaders_weighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader_generator_weighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel_weighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFixedWidthModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_evaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m result_dict_weighted = model_weighted.train(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mloaders_weighted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/prediction_utils/prediction_utils/pytorch_utils/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loaders, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m                         \u001b[0mthe_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform_batch_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                     )\n\u001b[0;32m--> 340\u001b[0;31m                     \u001b[0mloss_dict_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/prediction_utils/prediction_utils/pytorch_utils/models.py\u001b[0m in \u001b[0;36mforward_on_batch\u001b[0;34m(self, the_data)\u001b[0m\n\u001b[1;32m    261\u001b[0m             loss_dict_batch = {\n\u001b[1;32m    262\u001b[0m                 \"loss\": self.criterion(\n\u001b[0;32m--> 263\u001b[0;31m                     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthe_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 )\n\u001b[1;32m    265\u001b[0m             }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'weights'"
     ]
    }
   ],
   "source": [
    "# Weighted Model\n",
    "loader_generator_weighted = ArrayLoaderGenerator(\n",
    "    features=np.concatenate((features.astype(np.float32), df.group.values.reshape(-1, 1).astype(np.float32)), axis=1), \n",
    "    cohort=df.query('binary_event_indicator == 1'), \n",
    "    label_col='observed_labels',\n",
    "    fold_id=fold_id,\n",
    "    eval_key='val', \n",
    "    row_id_col='row_id', \n",
    "    weight_var_name='binary_censoring_weight',\n",
    "    num_workers=0,\n",
    "    **config_dict\n",
    ")\n",
    "\n",
    "loaders_weighted = loader_generator_weighted.init_loaders()\n",
    "model_weighted = FixedWidthModel(weighted_loss=True, weighted_evaluation=True, **config_dict)\n",
    "result_dict_weighted = model_weighted.train(\n",
    "    loaders_weighted,\n",
    "    num_epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation of weighted model\n",
    "loader_generator_predict = ArrayLoaderGenerator(\n",
    "    features=np.concatenate((features.astype(np.float32), df.group.values.reshape(-1, 1).astype(np.float32)), axis=1), \n",
    "    cohort=df, \n",
    "    fold_id=fold_id,\n",
    "    eval_key='test', \n",
    "    row_id_col='row_id', \n",
    "    label_col='observed_labels',\n",
    "    weight_var='binary_censoring_weight',\n",
    "    **config_dict\n",
    ")\n",
    "\n",
    "loaders_predict = loader_generator_predict.init_loaders_predict()\n",
    "\n",
    "predict_dict = model_weighted.predict(loaders_predict, phases=['test'])\n",
    "\n",
    "output_df_eval, result_df_eval = (\n",
    "    predict_dict[\"outputs\"],\n",
    "    predict_dict[\"performance\"],\n",
    ")\n",
    "\n",
    "output_df_eval = output_df_eval.drop(columns=['labels']).merge(\n",
    "    labels_df_dict['test'], \n",
    ").assign(labels = lambda x: x.labels.astype(np.long))\n",
    "\n",
    "display(temp_evaluate(df=output_df_eval, strata_vars=None))\n",
    "\n",
    "display(temp_evaluate(df=output_df_eval, strata_vars=['group']))\n",
    "\n",
    "display(temp_evaluate_fair(df=output_df_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Weighted Model - fair\n",
    "loader_generator_weighted = ArrayLoaderGenerator(\n",
    "    features=np.concatenate((features.astype(np.float32), df.group.values.reshape(-1, 1).astype(np.float32)), axis=1), \n",
    "    cohort=df.query('binary_event_indicator == 1'), \n",
    "    fold_id=fold_id,\n",
    "    eval_key='val', \n",
    "    row_id_col='row_id', \n",
    "    label_col='observed_labels',\n",
    "    weight_var='binary_censoring_weight',\n",
    "    num_workers=0,\n",
    "    **config_dict\n",
    ")\n",
    "\n",
    "loaders_weighted = loader_generator_weighted.init_loaders()\n",
    "\n",
    "model_class = group_lagrangian_model(\n",
    "    'multi'\n",
    ")\n",
    "\n",
    "model_weighted = model_class(\n",
    "    **config_dict, \n",
    "    **lagrangian_config,\n",
    "    logging_thresholds=lagrangian_config.get('thresholds'), \n",
    "    weighted_loss=True,\n",
    "    weighted_evaluation=True\n",
    ")\n",
    "result_dict_weighted = model_weighted.train(\n",
    "    loaders_weighted,\n",
    "    lr=1e-5,\n",
    "    num_epochs=20,\n",
    "    print_debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation of weighted fair model\n",
    "loader_generator_predict = ArrayLoaderGenerator(\n",
    "    features=np.concatenate((features.astype(np.float32), df.group.values.reshape(-1, 1).astype(np.float32)), axis=1), \n",
    "    cohort=df, \n",
    "    fold_id=fold_id,\n",
    "    eval_key='test', \n",
    "    row_id_col='row_id', \n",
    "    label_col='observed_labels',\n",
    "    weight_var='binary_censoring_weight',\n",
    "    **config_dict\n",
    ")\n",
    "\n",
    "loaders_predict = loader_generator_predict.init_loaders_predict()\n",
    "\n",
    "predict_dict = model_weighted.predict(loaders_predict, phases=['test'])\n",
    "\n",
    "output_df_eval, result_df_eval = (\n",
    "    predict_dict[\"outputs\"],\n",
    "    predict_dict[\"performance\"],\n",
    ")\n",
    "\n",
    "output_df_eval = output_df_eval.drop(columns=['labels']).merge(\n",
    "    labels_df_dict['test'], \n",
    ").assign(labels = lambda x: x.labels.astype(np.long))\n",
    "\n",
    "display(temp_evaluate(df=output_df_eval, strata_vars=None))\n",
    "\n",
    "display(temp_evaluate(df=output_df_eval, strata_vars=['group']))\n",
    "\n",
    "display(temp_evaluate_fair(df=output_df_eval))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
