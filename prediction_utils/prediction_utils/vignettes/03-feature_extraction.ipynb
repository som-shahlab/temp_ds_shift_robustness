{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Now that we have defined a cohort, we can extract features from the database for machine learning.\n",
    "\n",
    "This library provides the capability to efficiently extract a count-based feature representation from BigQuery. Here, we will explore some of those capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "from prediction_utils.extraction_utils.featurizer import BigQueryOMOPFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the extraction\n",
    "config_dict = {\n",
    "    \"gcloud_project\": \"som-nero-phi-nigam-starr\",\n",
    "    \"dataset_project\": \"som-rit-phi-starr-prod\",\n",
    "    \"rs_dataset_project\": \"som-nero-phi-nigam-starr\",\n",
    "    \"dataset\": \"starr_omop_cdm5_deid_1pcent_lite_latest\",\n",
    "    \"rs_dataset\": \"temp_dataset\",\n",
    "    \"data_path\": \"/share/pi/nigam/projects/prediction_utils/scratch/\",\n",
    "    \"features_by_analysis_path\": \"features_by_analysis\",\n",
    "    \"merged_name\": \"merged_features_binary\",\n",
    "    \"cohort_name\": \"vignette_cohort_filtered\",\n",
    "    \"row_id_field\": \"prediction_id\",\n",
    "    \"index_date_field\": \"admit_date\",\n",
    "    \"time_bins\": [-365, -90, 0],\n",
    "    \"include_all_history\": True,\n",
    "    \"overwrite\": True,\n",
    "    \"binary\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above configuration, we can now create a featurizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/envs/anaconda/envs/prediction_utils/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "featurizer = BigQueryOMOPFeaturizer(**config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The featurizer objects primary methods are `featurize` and `merge_features`. \n",
    "\n",
    "`featurize` performs a series of set of extractions labeled by `analysis_ids` and time bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see the list of analysis_ids that are currently defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['condition_occurrence',\n",
       " 'drug_exposure',\n",
       " 'device_exposure',\n",
       " 'measurement',\n",
       " 'procedure_occurrence',\n",
       " 'note_type',\n",
       " 'observation',\n",
       " 'note_nlp',\n",
       " 'measurement_range',\n",
       " 'measurement_bin',\n",
       " 'condition_occurrence_dt',\n",
       " 'drug_exposure_dt',\n",
       " 'device_exposure_dt',\n",
       " 'measurement_dt',\n",
       " 'procedure_occurrence_dt',\n",
       " 'note_type_dt',\n",
       " 'observation_dt',\n",
       " 'note_nlp_dt',\n",
       " 'measurement_range_dt',\n",
       " 'measurement_bin_dt',\n",
       " 'gender',\n",
       " 'race',\n",
       " 'ethnicity',\n",
       " 'age_group']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.valid_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these analyses can be binned over time. If the list of `time_bins` is defined, the extraction occurs separately over each time bin. If `include_all_history` is True, then the feature extraction will also occur separately over the full history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the featurizer, selecting only the analyses that we would like.\n",
    "Let's generate features for `gender`, `age_group`, `drug_exposure`, and `condition_occurrence`.\n",
    "Features for five time bins will be generated on the basis of the configuration we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/envs/anaconda/envs/prediction_utils/lib/python3.7/site-packages/pyarrow/util.py:39: FutureWarning: pyarrow.read_schema is deprecated as of 0.17.0, please use pyarrow.ipc.read_schema instead\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/share/pi/nigam/envs/anaconda/envs/prediction_utils/lib/python3.7/site-packages/pyarrow/util.py:39: FutureWarning: pyarrow.read_record_batch is deprecated as of 0.17.0, please use pyarrow.ipc.read_record_batch instead\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "featurizer.featurize(\n",
    "    analysis_ids=['gender', 'age_group', 'drug_exposure', 'condition_occurrence']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the results. The data was stored to `os.path.join(config_dict['data_path'], config_dict['features_by_analysis_path'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'age_group', 'drug_exposure', 'condition_occurrence']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(config_dict['data_path'], config_dict['features_by_analysis_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of gender\n",
      "['static']\n",
      "Contents of drug_exposure\n",
      "['bin_-36500_-1', 'bin_-90_-1', 'bin_-365_-91']\n",
      "Contents of drug_exposure/bin_-36500_-1\n",
      "['features_33.parquet']\n"
     ]
    }
   ],
   "source": [
    "print('Contents of gender')\n",
    "print(os.listdir(\n",
    "    os.path.join(\n",
    "        config_dict['data_path'], \n",
    "        config_dict['features_by_analysis_path'],\n",
    "        'gender'\n",
    "    )\n",
    "))\n",
    "print('Contents of drug_exposure')\n",
    "print(os.listdir(\n",
    "    os.path.join(\n",
    "        config_dict['data_path'], \n",
    "        config_dict['features_by_analysis_path'],\n",
    "        'drug_exposure'\n",
    "    )\n",
    "))\n",
    "print('Contents of drug_exposure/bin_-36500_-1')\n",
    "print(os.listdir(\n",
    "    os.path.join(\n",
    "        config_dict['data_path'], \n",
    "        config_dict['features_by_analysis_path'],\n",
    "        'drug_exposure',\n",
    "        'bin_-36500_-1'\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `merge_features` method can now be used to merge the individual features into a single feature representation. This also constructs a vocabulary of features that maps each unique feature to a unique numberic identifier from `0..vocab_size-1` where `vocab_size` is the number of unique features.\n",
    "\n",
    "The merge procedures can write the results to either Scipy CSR Sparse or Parquet dataset. For datasets that will fit in memory, CSR is recommended, and Parquet if the data is larger than memory.\n",
    "\n",
    "We will run the merge procedure, generating the CSR sparse result, and not the parquet, since the example data is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.merge_features(\n",
    "    create_sparse=True,\n",
    "    create_parquet=False,\n",
    "    existing_vocab_path=None,\n",
    "    **config_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vocab', 'features_sparse']\n",
      "Contents of vocab\n",
      "['vocab.parquet']\n",
      "Contents of features_sparse\n",
      "['features.gz', 'features_row_id_map.parquet']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.path.join(config_dict['data_path'], config_dict['merged_name'])))\n",
    "print('Contents of vocab')\n",
    "print(os.listdir(os.path.join(config_dict['data_path'], config_dict['merged_name'], 'vocab')))\n",
    "print('Contents of features_sparse')\n",
    "print(os.listdir(os.path.join(config_dict['data_path'], config_dict['merged_name'], 'features_sparse')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the results to explore the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = joblib.load(\n",
    "    os.path.join(config_dict['data_path'], config_dict['merged_name'], 'features_sparse', 'features.gz')\n",
    ")\n",
    "\n",
    "row_id_map = pd.read_parquet(\n",
    "    os.path.join(config_dict['data_path'], config_dict['merged_name'], 'features_sparse', 'features_row_id_map.parquet')\n",
    ")\n",
    "\n",
    "vocab = pd.read_parquet(\n",
    "    os.path.join(config_dict['data_path'], config_dict['merged_name'], 'vocab', 'vocab.parquet')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2059x16289 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 117540 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features_row_id</th>\n",
       "      <th>prediction_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3616212052160199172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2053957947170892260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-9204411047439856242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-6129485265169826587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-4401831474135185752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features_row_id        prediction_id\n",
       "0                0  3616212052160199172\n",
       "1                1  2053957947170892260\n",
       "2                2 -9204411047439856242\n",
       "3                3 -6129485265169826587\n",
       "4                4 -4401831474135185752"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_id_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_id</th>\n",
       "      <th>feature_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8532_gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8507_gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>age_group_10_age_group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>age_group_11_age_group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>age_group_12_age_group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_id              feature_id\n",
       "0       0             8532_gender\n",
       "1       1             8507_gender\n",
       "2       2  age_group_10_age_group\n",
       "3       3  age_group_11_age_group\n",
       "4       4  age_group_12_age_group"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prediction_utils [conda env:anaconda-prediction_utils]",
   "language": "python",
   "name": "conda-env-anaconda-prediction_utils-prediction_utils"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
